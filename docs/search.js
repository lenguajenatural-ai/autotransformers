window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "autotransformers", "modulename": "autotransformers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.augmentation", "modulename": "autotransformers.augmentation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.augmentation.TextAugmenterPipeline", "modulename": "autotransformers.augmentation.TextAugmenterPipeline", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.augmentation.TextAugmenterPipeline.NLPAugPipeline", "modulename": "autotransformers.augmentation.TextAugmenterPipeline", "qualname": "NLPAugPipeline", "kind": "class", "doc": "<p>Augment text data, with various forms of augmenting. It uses <code>nlpaug</code> in the background.</p>\n\n<p>The configuration of the augmentation pipeline is done with <code>autotransformers.augmentation.augmenter_config.NLPAugConfig</code>.\nNLPAugPipeline receives a list of configs of that type, where each config defines a type\nof augmentation technique to use, as well as the proportion of the train dataset that is\nto be augmented.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>steps: List[autotransformers.augmentation.augmenter_config.NLPAugConfig]\n    List of steps. Each step must be a NLPAugConfig instance.\ntext_field: str\n    Name of the field in the dataset where texts are.</p>\n"}, {"fullname": "autotransformers.augmentation.TextAugmenterPipeline.NLPAugPipeline.__init__", "modulename": "autotransformers.augmentation.TextAugmenterPipeline", "qualname": "NLPAugPipeline.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">steps</span>, </span><span class=\"param\"><span class=\"n\">text_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;text&#39;</span></span>)</span>"}, {"fullname": "autotransformers.augmentation.TextAugmenterPipeline.NLPAugPipeline.text_field", "modulename": "autotransformers.augmentation.TextAugmenterPipeline", "qualname": "NLPAugPipeline.text_field", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.augmentation.TextAugmenterPipeline.NLPAugPipeline.pipeline", "modulename": "autotransformers.augmentation.TextAugmenterPipeline", "qualname": "NLPAugPipeline.pipeline", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.augmentation.TextAugmenterPipeline.NLPAugPipeline.augment", "modulename": "autotransformers.augmentation.TextAugmenterPipeline", "qualname": "NLPAugPipeline.augment", "kind": "function", "doc": "<p>Augment data for datasets samples following the configuration defined at init.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>samples:\n    Samples from a datasets.Dataset</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>samples:\n    Samples from a datasets.Dataset but processed.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">samples</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.augmentation.augmenter_config", "modulename": "autotransformers.augmentation.augmenter_config", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.augmentation.augmenter_config.NLPAugConfig", "modulename": "autotransformers.augmentation.augmenter_config", "qualname": "NLPAugConfig", "kind": "class", "doc": "<p>Configuration for augmenters.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>name : str\n    Name of the data augmentation technique. Possible values currently are <code>ocr</code> (for OCR augmentation), <code>contextual_w_e</code>\n    for Contextual Word Embedding augmentation, <code>synonym</code>, <code>backtranslation</code>, <code>contextual_s_e</code> for Contextual Word Embeddings for Sentence Augmentation,\n    <code>abstractive_summ</code>. If using a custom augmenter class this can be a random name.\naugmenter_cls: Any\n    An optional augmenter class, from <code>nlpaug</code> library. Can be used instead of using an identifier name\n    for loading the class (see param <code>name</code> of this class).\nproportion : float\n    Proportion of data augmentation.\naug_kwargs : Dict\n    Arguments for the data augmentation class. See <a href=\"https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb\">https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb</a></p>\n"}, {"fullname": "autotransformers.augmentation.augmenter_config.NLPAugConfig.__init__", "modulename": "autotransformers.augmentation.augmenter_config", "qualname": "NLPAugConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">augmenter_cls</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">proportion</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">aug_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "autotransformers.augmentation.augmenter_config.NLPAugConfig.name", "modulename": "autotransformers.augmentation.augmenter_config", "qualname": "NLPAugConfig.name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "autotransformers.augmentation.augmenter_config.NLPAugConfig.augmenter_cls", "modulename": "autotransformers.augmentation.augmenter_config", "qualname": "NLPAugConfig.augmenter_cls", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.augmentation.augmenter_config.NLPAugConfig.proportion", "modulename": "autotransformers.augmentation.augmenter_config", "qualname": "NLPAugConfig.proportion", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.1"}, {"fullname": "autotransformers.augmentation.augmenter_config.NLPAugConfig.aug_kwargs", "modulename": "autotransformers.augmentation.augmenter_config", "qualname": "NLPAugConfig.aug_kwargs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict", "default_value": "None"}, {"fullname": "autotransformers.autotrainer", "modulename": "autotransformers.autotrainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.autotrainer.metric_func_map", "modulename": "autotransformers.autotrainer", "qualname": "metric_func_map", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;ner&#x27;: &lt;function compute_metrics_ner&gt;, &#x27;classification&#x27;: &lt;function compute_metrics_classification&gt;, &#x27;qa&#x27;: None, &#x27;seq2seq&#x27;: &lt;function compute_metrics_summarization&gt;, &#x27;multilabel&#x27;: &lt;function compute_metrics_multilabel&gt;, &#x27;chatbot&#x27;: None}"}, {"fullname": "autotransformers.autotrainer.AutoTrainer", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer", "kind": "class", "doc": "<p>Main class of autotransformers. Fine-tune and evaluate several models on several datasets.</p>\n\n<p>Useful for performing benchmarking of different models on the same datasets. The behavior\nof <code>AutoTrainer</code> is mainly configured through <code>model_configs</code> and <code>dataset_configs</code>, which\ndefine the datasets and the models to be used.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model_configs: List[autotransformers.ModelConfig]\n    Configurations for the models, instances of ModelConfig, each describing their\n    names in the hub or local directory, the name to save the model, the dropout\n    values to use, and a long etc.\ndataset_configs: List[autotransformers.DatasetConfig]\n    Configurations for the datasets, instances of DatasetConfig, each describing\n    how each dataset should be processed.\nmetrics_dir: str\n    Directory to save the metrics for the experiments, as returned by <code>autotransformers.ResultsGetter</code>.\nhp_search_mode: str\n    Mode for hyperparameter search; possibilities are <code>optuna</code> or <code>fixed</code>. If <code>fixed</code>,\n    no hyperparameter tuning is carried out.\nclean: bool\n    Whether to clean checkpoints every 10 minutes to avoid using too much disk, by\n    using autotransformers.CkptCleaner. Best model checkpoint is also saved when unuseful\n    checkpoints are deleted.\nmetrics_cleaner: str\n    Path to the folder where the metrics of the checkpoint cleaner should be stored.\n    These metrics are used to decide which checkpoints should be removed. Note: if the\n    experiment fails for some reason, and you re-launch it, please remove this folder\n    before doing so. Otherwise there will probably be an error, as the checkpoint cleaner\n    will use metrics from past experiments, not the running one, so there will be incorrect\n    checkpoint removals.\nuse_auth_token: bool\n    Whether to use auth token to load datasets and models.\nskip_mixes: List[autotransformers.SkipMix]\n    List of SkipMix instances with combinations of datasets and models that must be skipped.</p>\n"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.__init__", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_configs</span><span class=\"p\">:</span> <span class=\"n\">List</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_configs</span><span class=\"p\">:</span> <span class=\"n\">List</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;tmp_experiments_metrics&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">hp_search_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;optuna&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">clean</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cleaner</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;tmp_metrics_cleaner&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">use_auth_token</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">skip_mixes</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.model_configs", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.model_configs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.dataset_configs", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.dataset_configs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.metrics_dir", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.metrics_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.hp_search_mode", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.hp_search_mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.metrics_cleaner", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.metrics_cleaner", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.clean", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.clean", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.use_auth_token", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.use_auth_token", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.skip_mixes", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.skip_mixes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.train_with_fixed_params", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.train_with_fixed_params", "kind": "function", "doc": "<p>Train without hyperparameter search, with a fixed set of params.</p>\n\n<p>The default parameters are defined in the fixed_train_args of DatasetConfig.\nHowever, we can use ModelConfig.overwrite_training_args to change this,\nby passing a dictionary with the new parameters that we want to use for a model.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.optuna_hp_search", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.optuna_hp_search", "kind": "function", "doc": "<p>Carry out hyperparameter search with Optuna.</p>\n\n<p>Use <code>model_configs</code> and <code>dataset_configs</code> passed in init. Iterate over\neach dataset, and then over each model, with hyperparameter tuning.\nMetrics over the test dataset are gathered and then saved\nin the <code>metrics_dir</code> specified in init for each of those models, for later comparison.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>all_results: Dict\n    Dictionary with results from the experiments.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.train_one_model_fixed_params", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.train_one_model_fixed_params", "kind": "function", "doc": "<p>Train one model with fixed params in one dataset, without tuning parameters.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model_config: autotransformers.ModelConfig\n    Configuration for the model.\ndataset_config: autotransformers.DatasetConfig,\n    Configuration for the dataset.\ncompute_metrics_func: Any\n    Function to compute metrics.\ntest_dataset: datasets.Dataset\n    Test dataset to get metrics on.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>test_results: Dict\n    Dictionary with results over the test set after training with fixed params.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model_config</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span>,</span><span class=\"param\">\t<span class=\"n\">compute_metrics_func</span>,</span><span class=\"param\">\t<span class=\"n\">test_dataset</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.autotrainer.AutoTrainer.train_one_model_optuna", "modulename": "autotransformers.autotrainer", "qualname": "AutoTrainer.train_one_model_optuna", "kind": "function", "doc": "<p>Train one model in one dataset, with hyperparameter tuning, using Optuna.</p>\n\n<p>Load a checkpoint cleaner in the background to clean bad performing checkpoints\nevery 10 minutes, also saving the best performing checkpoint. Then, carry out\nhyperparameter search and, if configured (see <code>DatasetConfig</code>), retrain at end\nwith the best hyperparameters again. After that, results on the test set are\nobtained. For that, <code>ResultsGetter</code> is used for dataset processing, prediction\nand metrics gathering. If desired, the user may change the behavior\nof this part by creating a custom <code>ResultsGetter</code> overriding the desired\nmethods, and passing it to <code>DatasetConfig</code> as a <code>custom_results_getter</code>.\nMetrics are saved in json or txt format, and, if configured, the model\nis pushed to the hub.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model_config: autotransformers.ModelConfig\n    Configuration for the model.\ndataset_config: autotransformers.DatasetConfig,\n    Configuration for the dataset.\ncompute_objective: Any\n    Function to return the computed metric objective.\ncompute_metrics_func: Any\n    Function to compute metrics.\noutput_dir: str\n    Directory where the model is saved.\ntest_dataset: datasets.Dataset\n    Test dataset to get metrics on.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>test_results: Dict\n    Dictionary with the results in the test set.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model_config</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span>,</span><span class=\"param\">\t<span class=\"n\">compute_objective</span>,</span><span class=\"param\">\t<span class=\"n\">compute_metrics_func</span>,</span><span class=\"param\">\t<span class=\"n\">output_dir</span>,</span><span class=\"param\">\t<span class=\"n\">test_dataset</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.ckpt_cleaner", "modulename": "autotransformers.ckpt_cleaner", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner", "kind": "class", "doc": "<p>Clean all checkpoints that are no longer useful.</p>\n\n<p>Use a metrics dictionary to check the results of all runs of a model\nfor a dataset, then sort these metrics to decide which checkpoints are\nremovable and which are among the four best. When called, only those\nare kept, and all the other checkpoints are removed. This enables the\nuser to effectively use their computer resources, so there is no need to\nworry about the disk usage, which is a typical concern when running multiple\ntransformer models.</p>\n"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.__init__", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">current_folder_clean</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">current_dataset_folder</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">modelname</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;max&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">try_mode</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.current_folder_clean", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.current_folder_clean", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.current_dataset_folder", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.current_dataset_folder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.modelname", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.modelname", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.metrics_save_dir", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.metrics_save_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.mode", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.try_mode", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.try_mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.last_saved_ckpt", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.last_saved_ckpt", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.get_best_name", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.get_best_name", "kind": "function", "doc": "<p>Get the path of the best performing model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>metrics: Dict\n    Metrics of all models in a dictionary.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>best: str\n    Path to the best performing model.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">metrics</span><span class=\"p\">:</span> <span class=\"n\">Dict</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.save_best", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.save_best", "kind": "function", "doc": "<p>Save best model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>best_model: str\n    Path of the best performing model.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>target: str\n    Complete path to the target directory where the best model has been copied.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">best_model</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.fix_dir", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.fix_dir", "kind": "function", "doc": "<p>Fix directory path for windows file systems.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dir: str\n    Directory to fix.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dir: str\n    Fixed directory.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.ckpt_cleaner.CkptCleaner.remove_dirs", "modulename": "autotransformers.ckpt_cleaner", "qualname": "CkptCleaner.remove_dirs", "kind": "function", "doc": "<p>Delete checkpoint directories.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>checkpoint_dirs: List\n    List with the checkpoint directories to remove.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">checkpoint_dirs</span><span class=\"p\">:</span> <span class=\"n\">List</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.dataset_config", "modulename": "autotransformers.dataset_config", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.dataset_config.DatasetConfig", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig", "kind": "class", "doc": "<p>Configure a dataset for use within the AutoTrainer class.</p>\n\n<p>This determines how to load the dataset,\nwhether local files are needed, whether additional splits are needed (for example when the original\ndataset only has train-test and we want also validation), and so on.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset_name: str\n    The name of the dataset.\nalias: str\n    Alias for the dataset, for saving it.\ntask: str\n    The task of the dataset. Currenlty, only classification, ner and qa (question answering) are available.\nfixed_training_args: Dict\n    The training arguments (to use in transformers.TrainingArguments) for every model on this dataset, in dictionary format.\nis_multilabel: bool\n    Whether it is multilabel classification\nmultilabel_label_names: List\n    Names of the labels for multilabel training.\nhf_load_kwargs: Dict\n    Arguments for loading the dataset from the huggingface datasets' hub. Example: {'path': 'wikiann', 'name': 'es'}.\n    If None, it is assumed that all necessary files exist locally and are passed in the files field.\ntype_load: str\n    The type of load to perform in load_dataset; for example, if your data is in csv format (d = load_dataset('csv', ...)), this should be csv.\nfiles: Dict\n    Files to load the dataset from, in Huggingface's datasets format. Possible keys are train, validation and test.\ndata_field: str\n    Field to load data from in the case of jsons loading in datasets.\npartial_split: bool\n    Wheter a partial split is needed, that is, if you only have train and test sets, this should be True so that a new validation set is created.\nsplit: bool\n    This should be true when you only have one split, that is, a big train set; this creates new validation and test sets.\nlabel_col: str\n    Name of the label column.\nval_size: float\n    In case no validation split is provided, the proportion of the training data to leave for validation.\ntest_size: float\n    In case no test split is provided, the proportion of the total data to leave for testing.\npre_func\n    Function to perform previous transformations. For example, if your dataset lacks a field (like xquad with title field for example), you can fix it in a function provided here.\nsquad_v2: bool\n    Only useful for question answering. Whether it is squad v2 format or not. Default is false.\ntext_field: str\n    The name of the field containing the text. Useful only in case of unique-text-field datasets,like most datasets are. In case of 2-sentences datasets like xnli or paws-x this is not useful. Default is text.\nis_2sents: bool\n    Whether it is a 2 sentence dataset. Useful for processing datasets like xnli or paws-x.\nsentence1_field: str\n    In case this is a 2 sents dataset, the name of the first sentence field.\nsentence2_field: str\n    In case this is a 2 sents dataset, the name of the second sentence field.\nsummary_field: str = field(\n    The name of the field with summaries (we assume the long texts are in the text_field field). Only useful for summarization tasks. Default is summary.\ncallbacks: List\n    Callbacks to use inside transformers.\nmetric_optimize: str\n    Name of the metric you want to optimize in the hyperparameter search.\ndirection_optimize : str\n    Direction of the optimization problem. Whether you want to maximize or minimize metric_optimize.\ncustom_eval_func: Any\n    In case we want a special evaluation function, we can provide it here. It must receive EvalPredictions by trainer, like any compute_metrics function in transformers.\nseed : int\n    Seed for optuna sampler.\nmax_length_summary: int\n    Max length of the summaries, for tokenization purposes. It will be changed depending on the ModelConfig.\nnum_proc : int\n    Number of processes to preprocess data.\nloaded_dataset: Any\n    In case you want to do weird things like concatenating datasets or things like that, you can do that here, by passing a (non-tokenized) dataset in this field.\nadditional_metrics: List\n    List of additional metrics loaded from datasets, to compute over the test part.\nretrain_at_end: bool\n    whether to retrain with the best performing model. In most cases this should be True, except when training 1 model with 1 set of hyperparams.\nconfig_num_labels: int\n    Number of labels to set for the config, if None it will be computed based on number of labels detected.\nsmoke_test: bool\n    Whether to select only top 10 rows of the dataset for smoke testing purposes.\naugment_data: bool\n    Whether to augment_data or not.\ndata_augmentation_steps: List\n    List of data augmentation techniques to use from NLPAugPipeline.\npretokenized_dataset: Any\n    Pre-tokenized dataset, to avoid tokenizing inside AutoTrainer, which may cause memory issues with huge datasets.</p>\n\n<h2 id=\"examples\">Examples</h2>\n\n<p>One can easily create a DatasetConfig for dataset conll2002 just with the following:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">autotransformers</span> <span class=\"kn\">import</span> <span class=\"n\">DatasetConfig</span>\n</code></pre>\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">config</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;fixed_training_args&#39;</span><span class=\"p\">:</span> <span class=\"p\">{},</span> <span class=\"s1\">&#39;dataset_name&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;conll2002&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;alias&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;conll2002&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;task&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;ner&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;hf_load_kwargs&#39;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">&#39;path&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;conll2002&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;name&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;es&#39;</span><span class=\"p\">},</span> <span class=\"s1\">&#39;label_col&#39;</span><span class=\"p\">:</span><span class=\"s1\">&#39;ner_tags&#39;</span><span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">DatasetConfig</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">config</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.__init__", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">alias</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_training_args</span><span class=\"p\">:</span> <span class=\"n\">Dict</span>,</span><span class=\"param\">\t<span class=\"n\">is_multilabel</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">multilabel_label_names</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">factory</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">hf_load_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">type_load</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;json&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">files</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">data_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;data&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">partial_split</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">label_col</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;label_list&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">val_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.15</span>,</span><span class=\"param\">\t<span class=\"n\">test_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.15</span>,</span><span class=\"param\">\t<span class=\"n\">pre_func</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">remove_fields_pre_func</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">squad_v2</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">text_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;text&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">is_2sents</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">sentence1_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sentence2_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">summary_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;summary&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">factory</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">metric_optimize</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;eval_loss&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">direction_optimize</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;minimize&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">custom_eval_func</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">420</span>,</span><span class=\"param\">\t<span class=\"n\">max_length_summary</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">120</span>,</span><span class=\"param\">\t<span class=\"n\">num_proc</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">loaded_dataset</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">additional_metrics</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">retrain_at_end</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">config_num_labels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">smoke_test</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">augment_data</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">data_augmentation_steps</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">factory</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">id_field_qa</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;id&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">pretokenized_dataset</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_config_problem_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">chat_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;messages&#39;</span></span>)</span>"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.dataset_name", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.dataset_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.alias", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.alias", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.task", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.task", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.fixed_training_args", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.fixed_training_args", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.is_multilabel", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.is_multilabel", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.multilabel_label_names", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.multilabel_label_names", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.hf_load_kwargs", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.hf_load_kwargs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.type_load", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.type_load", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;json&#x27;"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.files", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.files", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.data_field", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.data_field", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;data&#x27;"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.partial_split", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.partial_split", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.split", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.split", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.label_col", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.label_col", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;label_list&#x27;"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.val_size", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.val_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.15"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.test_size", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.test_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.15"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.pre_func", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.pre_func", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.remove_fields_pre_func", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.remove_fields_pre_func", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.squad_v2", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.squad_v2", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.text_field", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.text_field", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;text&#x27;"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.is_2sents", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.is_2sents", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.sentence1_field", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.sentence1_field", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.sentence2_field", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.sentence2_field", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.summary_field", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.summary_field", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;summary&#x27;"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.callbacks", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.callbacks", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.metric_optimize", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.metric_optimize", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;eval_loss&#x27;"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.direction_optimize", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.direction_optimize", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;minimize&#x27;"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.custom_eval_func", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.custom_eval_func", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.seed", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.seed", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "420"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.max_length_summary", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.max_length_summary", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "120"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.num_proc", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.num_proc", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "4"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.loaded_dataset", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.loaded_dataset", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.additional_metrics", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.additional_metrics", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.retrain_at_end", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.retrain_at_end", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.config_num_labels", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.config_num_labels", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.smoke_test", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.smoke_test", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.augment_data", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.augment_data", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.data_augmentation_steps", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.data_augmentation_steps", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.id_field_qa", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.id_field_qa", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;id&#x27;"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.pretokenized_dataset", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.pretokenized_dataset", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.model_config_problem_type", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.model_config_problem_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "None"}, {"fullname": "autotransformers.dataset_config.DatasetConfig.chat_field", "modulename": "autotransformers.dataset_config", "qualname": "DatasetConfig.chat_field", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;messages&#x27;"}, {"fullname": "autotransformers.default_param_spaces", "modulename": "autotransformers.default_param_spaces", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.default_param_spaces.hp_space_base", "modulename": "autotransformers.default_param_spaces", "qualname": "hp_space_base", "kind": "function", "doc": "<p>Hyperparameter space in Optuna format for base-sized models (e.g. bert-base).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trial</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.default_param_spaces.hp_space_large", "modulename": "autotransformers.default_param_spaces", "qualname": "hp_space_large", "kind": "function", "doc": "<p>Hyperparameter space in Optuna format for large-sized models (e.g. bert-large).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">trial</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.hfdatasets_manager", "modulename": "autotransformers.hfdatasets_manager", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.hfdatasets_manager.tok_func_map", "modulename": "autotransformers.hfdatasets_manager", "qualname": "tok_func_map", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;ner&#x27;: &lt;function tokenize_ner&gt;, &#x27;qa&#x27;: &lt;function tokenize_squad&gt;, &#x27;seq2seq&#x27;: &lt;function tokenize_summarization&gt;, &#x27;classification&#x27;: &lt;function tokenize_classification&gt;, &#x27;chatbot&#x27;: &lt;function tokenize_chatbot&gt;, &#x27;alignment&#x27;: None}"}, {"fullname": "autotransformers.hfdatasets_manager.HFDatasetsManager", "modulename": "autotransformers.hfdatasets_manager", "qualname": "HFDatasetsManager", "kind": "class", "doc": "<p>Utility for loading HF Datasets' objects, using a DatasetConfig and a ModelConfig.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset_config: autotransformers.DatasetConfig\n    Configuration for the dataset\nmodel_config: autotransformers.ModelConfig\n    Configuration for the model.</p>\n"}, {"fullname": "autotransformers.hfdatasets_manager.HFDatasetsManager.__init__", "modulename": "autotransformers.hfdatasets_manager", "qualname": "HFDatasetsManager.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset_config</span>, </span><span class=\"param\"><span class=\"n\">model_config</span></span>)</span>"}, {"fullname": "autotransformers.hfdatasets_manager.HFDatasetsManager.dataset_config", "modulename": "autotransformers.hfdatasets_manager", "qualname": "HFDatasetsManager.dataset_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.hfdatasets_manager.HFDatasetsManager.model_config", "modulename": "autotransformers.hfdatasets_manager", "qualname": "HFDatasetsManager.model_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.hfdatasets_manager.HFDatasetsManager.get_dataset_and_tag2id", "modulename": "autotransformers.hfdatasets_manager", "qualname": "HFDatasetsManager.get_dataset_and_tag2id", "kind": "function", "doc": "<p>Get dataset and tag2id depending on dataset and model config.</p>\n\n<p>Using dataset config (task, etc), a preprocessing is applied to\nthe dataset, tokenizing text data, returning a processed dataset\nready for the configured task.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tokenizer: transformers.PretrainedTokenizer\n    Tokenizer to process data.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dataset: datasets.DatasetDict\n    Tokenized dataset.\ntag2id: Dict\n    Dictionary with tags (labels) and their indexes.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span><span class=\"p\">:</span> <span class=\"n\">transformers</span><span class=\"o\">.</span><span class=\"n\">tokenization_utils</span><span class=\"o\">.</span><span class=\"n\">PreTrainedTokenizer</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.hftransformers_manager", "modulename": "autotransformers.hftransformers_manager", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.hftransformers_manager.MultilabelTrainer", "modulename": "autotransformers.hftransformers_manager", "qualname": "MultilabelTrainer", "kind": "class", "doc": "<p>Version of the trainer used for multilabel setting.</p>\n", "bases": "transformers.trainer.Trainer"}, {"fullname": "autotransformers.hftransformers_manager.MultilabelTrainer.compute_loss", "modulename": "autotransformers.hftransformers_manager", "qualname": "MultilabelTrainer.compute_loss", "kind": "function", "doc": "<p>Compute loss of the model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model : transformers.PreTrainedModel\n    Model to compute loss.\ninputs : torch.Tensor\n    Model inputs.\nreturn_outputs : bool\n    Wether or not to return model outputs.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">inputs</span>, </span><span class=\"param\"><span class=\"n\">return_outputs</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.hftransformers_manager.map_trainer_cls", "modulename": "autotransformers.hftransformers_manager", "qualname": "map_trainer_cls", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;classification&#x27;: &lt;class &#x27;transformers.trainer.Trainer&#x27;&gt;, &#x27;ner&#x27;: &lt;class &#x27;transformers.trainer.Trainer&#x27;&gt;, &#x27;qa&#x27;: &lt;class &#x27;transformers.trainer.Trainer&#x27;&gt;, &#x27;chatbot&#x27;: &lt;class &#x27;transformers.trainer.Trainer&#x27;&gt;, &#x27;multilabel&#x27;: &lt;class &#x27;autotransformers.hftransformers_manager.MultilabelTrainer&#x27;&gt;, &#x27;seq2seq&#x27;: &lt;class &#x27;transformers.trainer_seq2seq.Seq2SeqTrainer&#x27;&gt;, &#x27;alignment&#x27;: &lt;class &#x27;trl.trainer.dpo_trainer.DPOTrainer&#x27;&gt;}"}, {"fullname": "autotransformers.hftransformers_manager.map_model_cls", "modulename": "autotransformers.hftransformers_manager", "qualname": "map_model_cls", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;classification&#x27;: &lt;class &#x27;transformers.models.auto.modeling_auto.AutoModelForSequenceClassification&#x27;&gt;, &#x27;ner&#x27;: &lt;class &#x27;transformers.models.auto.modeling_auto.AutoModelForTokenClassification&#x27;&gt;, &#x27;qa&#x27;: &lt;class &#x27;transformers.models.auto.modeling_auto.AutoModelForQuestionAnswering&#x27;&gt;, &#x27;seq2seq&#x27;: &lt;class &#x27;transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM&#x27;&gt;, &#x27;chatbot&#x27;: &lt;class &#x27;transformers.models.auto.modeling_auto.AutoModelForCausalLM&#x27;&gt;, &#x27;alignment&#x27;: &lt;class &#x27;transformers.models.auto.modeling_auto.AutoModelForCausalLM&#x27;&gt;}"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager", "kind": "class", "doc": "<p>Utility for loading HF Transformers' objects, using a dataset config and a model config.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model_config: autotransformers.ModelConfig\n    Configuration for the model.\ndataset_config: autotransformers.DatasetConfig\n    Configuration for the dataset</p>\n"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.__init__", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_config</span><span class=\"p\">:</span> <span class=\"n\">autotransformers</span><span class=\"o\">.</span><span class=\"n\">model_config</span><span class=\"o\">.</span><span class=\"n\">ModelConfig</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">autotransformers</span><span class=\"o\">.</span><span class=\"n\">dataset_config</span><span class=\"o\">.</span><span class=\"n\">DatasetConfig</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_auth_token</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.model_config", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.model_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.dataset_config", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.dataset_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.use_auth_token", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.use_auth_token", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.load_config", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.load_config", "kind": "function", "doc": "<p>Load configuration for the model depending on the type of task we are doing.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tag2id: Dict\n    Dictionary mapping labels to indices of those labels in the network output layer.\ndropout: float\n    Dropout percentage to apply at classification head.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>config: transformers.PretrainedConfig\n    Configuration for use in the transformers module.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">tag2id</span><span class=\"p\">:</span> <span class=\"n\">Dict</span>, </span><span class=\"param\"><span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.load_data_collator", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.load_data_collator", "kind": "function", "doc": "<p>Load data collator depending on the type of task we are doing.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tokenizer: transformers.PretrainedTokenizer\n    Tokenizer to process data.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>data_collator: transformers.DataCollator\n    DataCollator for use in the transformers library.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.load_tokenizer", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.load_tokenizer", "kind": "function", "doc": "<p>Load tokenizer for the given model config and model name.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tokenizer:\n    Loaded tokenizer.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.get_model_cls", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.get_model_cls", "kind": "function", "doc": "<p>Get the class to use for a model.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>model_cls:\n    Class for the model.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.load_trainer", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.load_trainer", "kind": "function", "doc": "<p>Load an instantiated Trainer object depending on the configuration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset: datasets.DatasetDict\n    Dataset with train and validation splits.\ntokenizer: transformers.PretrainedTokenizer\n    Tokenizer from transformers.\nargs: transformers.TrainingArguments\n    TrainingArguments for the Trainer.\nmodel_init: Any\n    Function that loads the model.\ndata_collator: Any\n    Data Collator to use inside Trainer.\ncompute_metrics_func: Any\n    Function to compute metrics.\nconfig: transformers.PretrainedConfig\n    Configuration for the model in Huggingface Transformers.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Trainer: transformers.Trainer\n    Trainer object loaded with the given configuration.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span>,</span><span class=\"param\">\t<span class=\"n\">tokenizer</span>,</span><span class=\"param\">\t<span class=\"n\">args</span>,</span><span class=\"param\">\t<span class=\"n\">model_init</span>,</span><span class=\"param\">\t<span class=\"n\">data_collator</span>,</span><span class=\"param\">\t<span class=\"n\">compute_metrics_func</span>,</span><span class=\"param\">\t<span class=\"n\">config</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.load_train_args", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.load_train_args", "kind": "function", "doc": "<p>Load training args depending on the task.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>output_dir: str\n    Local directory name to save the model.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>args: transformers.TrainingArguments\n    Arguments for training.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">output_dir</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.hftransformers_manager.HFTransformersManager.load_model_init", "modulename": "autotransformers.hftransformers_manager", "qualname": "HFTransformersManager.load_model_init", "kind": "function", "doc": "<p>Load the model init function.</p>\n\n<p>This function is useful for the Transformers integration with Optuna.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model_cls\n    Class for the model.\nconfig: AutoConfig\n    Configuration for the model.\ntokenizer: transformers.PretrainedTokenizer\n    Tokenizer to preprocess text data.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>model_init\n    Function for initializing the model. Furtherly passed to the Trainer.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model_cls</span>, </span><span class=\"param\"><span class=\"n\">config</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.llm_templates", "modulename": "autotransformers.llm_templates", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.llm_templates.instructions_to_chat", "modulename": "autotransformers.llm_templates", "qualname": "instructions_to_chat", "kind": "function", "doc": "<p>Processes a single sample from any dataset to structure it for chatbot training or instruction-based tasks,\nsupporting nested structures and optional fields with a more Pythonic approach.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>sample : dict\n    A dictionary representing a single sample from the dataset.\ninput_field : str\n    The key for the user's input.\noutput_field : str\n    The key for the assistant's output.\ncontext_field : str, optional\n    The key for additional context, if any.\nnested_field : str, optional\n    The key for a nested field within the sample, if the data structure is nested.\nsystem_message : str, optional\n    A system message to initialize the conversation. If None, a default message is used.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dict\n    A modified dictionary with a 'messages' key containing ordered messages,\n    each annotated with its role in the conversation.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sample</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>,</span><span class=\"param\">\t<span class=\"n\">input_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">context_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">nested_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">system_message</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.llm_templates.neftune_forward", "modulename": "autotransformers.llm_templates", "qualname": "neftune_forward", "kind": "function", "doc": "<p>Implement the NEFTune forward pass for the model. Note this works only for torch.nn.Embedding layers.</p>\n\n<p>This method is slightly adapted from the original source code that can be found here: <a href=\"https://github.com/neelsjain/NEFTune\">https://github.com/neelsjain/NEFTune</a></p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input (<code>torch.Tensor</code>):\n    The input tensor to the model.\nnoise_alpha (<code>float</code>):\n    The noise alpha value to use for the NEFTune forward pass.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>embeddings: torch.Tensor\n    Embeddings with random noise added.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.llm_templates.activate_neftune", "modulename": "autotransformers.llm_templates", "qualname": "activate_neftune", "kind": "function", "doc": "<p>Activates Neftune noise injection in the input embeddings of a given model by replacing its original forward method with a custom forward method that includes Neftune noise.</p>\n\n<p>This function supports models of type PreTrainedModel and PeftModel. It modifies the model in-place by adding Neftune noise with a specified alpha value to the input embeddings and keeps a reference to the original forward method.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model : Union[PreTrainedModel, PeftModel]\n    The model to modify. It should be an instance of either PreTrainedModel or PeftModel.\nneftune_noise_alpha : float, optional\n    The alpha value for the Neftune noise to be applied. It controls the intensity of the noise injected into the input embeddings. Default value is 5.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>model : Union[PreTrainedModel, PeftModel]\n    The modified model with Neftune noise injection activated in its input embeddings.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<ul>\n<li>The activation of Neftune noise involves modifying the forward pass of the model's input embeddings to include noise injection.</li>\n<li>This function retains a reference to the original forward method of the embeddings, allowing for potential restoration if needed.</li>\n<li>The technique used to replace the forward method is based on a discussion from PyTorch's forums, acknowledging the complexity and hacky nature of this operation.</li>\n</ul>\n\n<h2 id=\"references\">References</h2>\n\n<ul>\n<li>PyTorch Forum Discussion: <a href=\"https://discuss.pytorch.org/t/how-can-i-replace-the-forward-method-of-a-predefined-torchvision-model-with-my-customized-forward-function/54224/11\">https://discuss.pytorch.org/t/how-can-i-replace-the-forward-method-of-a-predefined-torchvision-model-with-my-customized-forward-function/54224/11</a></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">neftune_noise_alpha</span><span class=\"o\">=</span><span class=\"mi\">5</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.llm_templates.NEFTuneTrainer", "modulename": "autotransformers.llm_templates", "qualname": "NEFTuneTrainer", "kind": "class", "doc": "<p>A custom trainer class for integrating Neftune noise into the training process of models derived from PreTrainedModel or PeftModel.</p>\n\n<p>This trainer extends the functionality of the <code>Trainer</code> class by allowing the injection of Neftune noise into the input embeddings\nduring training, and ensures the original forward method is restored after training completes.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>neftune_noise_alpha : float, optional\n    The alpha value for the Neftune noise to be applied. It controls the intensity of the noise injected into the input embeddings.\n    If None, Neftune noise injection is not activated. Default is None.\n<em>args : variable length argument list\n    Arguments passed to the <code>Trainer</code> class initializer.\n*</em>kwargs : arbitrary keyword arguments\n    Keyword arguments passed to the <code>Trainer</code> class initializer.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>train(<em>args, *</em>kwargs)\n    Extends the <code>Trainer.train</code> method by injecting Neftune noise into the model's input embeddings before training and restoring\n    the original forward pass method of the embeddings after training.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<ul>\n<li>It is important to ensure that the <code>neftune_noise_alpha</code> is set appropriately to avoid excessively distorting the input embeddings.</li>\n<li>The restoration of the original forward method after training is crucial for maintaining the expected behavior of the model outside of training.</li>\n</ul>\n", "bases": "transformers.trainer.Trainer"}, {"fullname": "autotransformers.llm_templates.NEFTuneTrainer.__init__", "modulename": "autotransformers.llm_templates", "qualname": "NEFTuneTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">neftune_noise_alpha</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "autotransformers.llm_templates.NEFTuneTrainer.neftune_noise_alpha", "modulename": "autotransformers.llm_templates", "qualname": "NEFTuneTrainer.neftune_noise_alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.llm_templates.NEFTuneTrainer.train", "modulename": "autotransformers.llm_templates", "qualname": "NEFTuneTrainer.train", "kind": "function", "doc": "<p>Main training entry point.</p>\n\n<p>Args:\n    resume_from_checkpoint (<code>str</code> or <code>bool</code>, <em>optional</em>):\n        If a <code>str</code>, local path to a saved checkpoint as saved by a previous instance of [<code>Trainer</code>]. If a\n        <code>bool</code> and equals <code>True</code>, load the last checkpoint in <em>args.output_dir</em> as saved by a previous instance\n        of [<code>Trainer</code>]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n    trial (<code>optuna.Trial</code> or <code>Dict[str, Any]</code>, <em>optional</em>):\n        The trial run or the hyperparameter dictionary for hyperparameter search.\n    ignore_keys_for_eval (<code>List[str]</code>, <em>optional</em>)\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions for evaluation during the training.\n    kwargs (<code>Dict[str, Any]</code>, <em>optional</em>):\n        Additional keyword arguments used to hide deprecated arguments</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">resume_from_checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">trial</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">optuna</span><span class=\"o\">.</span><span class=\"n\">trial</span><span class=\"o\">.</span><span class=\"n\">_trial</span><span class=\"o\">.</span><span class=\"n\">Trial</span><span class=\"p\">,</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_keys_for_eval</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.llm_templates.QLoraWrapperModelInit", "modulename": "autotransformers.llm_templates", "qualname": "QLoraWrapperModelInit", "kind": "class", "doc": "<p>A wrapper class for initializing transformer-based models with QLoRa and gradient checkpointing.</p>\n\n<p>This class serves as a wrapper for the <code>model_init</code> function, which initializes the model.\nIt activates gradient checkpointing when possible and applies QLoRa to the model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model_init : callable\n    A function that initializes the transformer-based model for training.\nmodel_config : Any\n    The configuration for the model.\ntokenizer : Any\n    The tokenizer used for tokenization.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Pre-trained model with QLoRa and gradient checkpointing, if enabled.</p>\n"}, {"fullname": "autotransformers.llm_templates.QLoraWrapperModelInit.__init__", "modulename": "autotransformers.llm_templates", "qualname": "QLoraWrapperModelInit.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_init</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">model_config</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span>)</span>"}, {"fullname": "autotransformers.llm_templates.QLoraWrapperModelInit.model_init", "modulename": "autotransformers.llm_templates", "qualname": "QLoraWrapperModelInit.model_init", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.llm_templates.QLoraWrapperModelInit.model_config", "modulename": "autotransformers.llm_templates", "qualname": "QLoraWrapperModelInit.model_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.llm_templates.QLoraWrapperModelInit.tokenizer", "modulename": "autotransformers.llm_templates", "qualname": "QLoraWrapperModelInit.tokenizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.llm_templates.QLoraWrapperModelInit.change_layer_types_for_stability", "modulename": "autotransformers.llm_templates", "qualname": "QLoraWrapperModelInit.change_layer_types_for_stability", "kind": "function", "doc": "<p>Change layer types of the model for stability.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model : PreTrainedModel\n    The pre-trained model.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Pre-trained model with modified layer types for stability.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">transformers</span><span class=\"o\">.</span><span class=\"n\">modeling_utils</span><span class=\"o\">.</span><span class=\"n\">PreTrainedModel</span></span><span class=\"return-annotation\">) -> <span class=\"n\">transformers</span><span class=\"o\">.</span><span class=\"n\">modeling_utils</span><span class=\"o\">.</span><span class=\"n\">PreTrainedModel</span>:</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.llm_templates.modify_tokenizer", "modulename": "autotransformers.llm_templates", "qualname": "modify_tokenizer", "kind": "function", "doc": "<p>Modify properties of a pre-trained tokenizer.</p>\n\n<p>This function allows you to modify various properties of a pre-trained tokenizer,\nsuch as the pad_token_id, padding_side, model_max_length, special tokens, and additional tokens.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tokenizer : PreTrainedTokenizer\n    The pre-trained tokenizer to be modified.\npad_token_id : int, optional (default=None)\n    The ID of the padding token to set for the tokenizer.\npadding_side : str, optional (default=None)\n    The side (either 'left' or 'right') to apply padding.\nnew_model_seq_length : int, optional (default=None)\n    The new maximum sequence length allowed by the tokenizer.\nadd_special_tokens : dict, optional (default=None)\n    A dictionary specifying special tokens to be added to the tokenizer.\nadd_tokens : list, optional (default=None)\n    A list of additional tokens to be added to the tokenizer's vocabulary.\nchat_template : str, optional (default=None)\n    The chat template to use for the tokenizer.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>PreTrainedTokenizer\n    The modified pre-trained tokenizer.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokenizer</span><span class=\"p\">:</span> <span class=\"n\">transformers</span><span class=\"o\">.</span><span class=\"n\">tokenization_utils</span><span class=\"o\">.</span><span class=\"n\">PreTrainedTokenizer</span>,</span><span class=\"param\">\t<span class=\"n\">pad_token_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">padding_side</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">new_model_seq_length</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">add_special_tokens</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">add_tokens</span><span class=\"p\">:</span> <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">chat_template</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">transformers</span><span class=\"o\">.</span><span class=\"n\">tokenization_utils</span><span class=\"o\">.</span><span class=\"n\">PreTrainedTokenizer</span>:</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.llm_templates.qlora_config", "modulename": "autotransformers.llm_templates", "qualname": "qlora_config", "kind": "variable", "doc": "<p></p>\n", "default_value": "BitsAndBytesConfig {\n  &quot;_load_in_4bit&quot;: true,\n  &quot;_load_in_8bit&quot;: false,\n  &quot;bnb_4bit_compute_dtype&quot;: &quot;bfloat16&quot;,\n  &quot;bnb_4bit_quant_type&quot;: &quot;nf4&quot;,\n  &quot;bnb_4bit_use_double_quant&quot;: true,\n  &quot;llm_int8_enable_fp32_cpu_offload&quot;: false,\n  &quot;llm_int8_has_fp16_weight&quot;: false,\n  &quot;llm_int8_skip_modules&quot;: null,\n  &quot;llm_int8_threshold&quot;: 6.0,\n  &quot;load_in_4bit&quot;: true,\n  &quot;load_in_8bit&quot;: false,\n  &quot;quant_method&quot;: &quot;bitsandbytes&quot;\n}\n"}, {"fullname": "autotransformers.llm_templates.SavePeftModelCallback", "modulename": "autotransformers.llm_templates", "qualname": "SavePeftModelCallback", "kind": "class", "doc": "<p>Callback for saving only adapter layers from PEFT (Parameter Efficient Fine-Tuning) checkpoints.</p>\n\n<p>This callback is designed to be applied to the <code>transformers.Trainer</code> to save adapter layers\nfrom PEFT checkpoints instead of saving full model weights.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>save_model(args, state, kwargs):\n    Save the adapter model from the PEFT checkpoint.\non_save(args, state, control, <em>*kwargs):\n    Triggered when saving the model during training. Calls <code>save_model</code> and returns control.\non_train_end(args, state, control, *</em>kwargs):\n    Triggered at the end of training. Creates a 'completed' file and calls <code>save_model</code>.</p>\n", "bases": "transformers.trainer_callback.TrainerCallback"}, {"fullname": "autotransformers.llm_templates.SavePeftModelCallback.save_model", "modulename": "autotransformers.llm_templates", "qualname": "SavePeftModelCallback.save_model", "kind": "function", "doc": "<p>Save the adapter model from the PEFT checkpoint.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>args : TrainerArguments\n    Arguments for the trainer.\nstate : TrainerState\n    Current trainer state.\nkwargs : dict\n    Additional keyword arguments, including the model.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>None</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"n\">state</span>, </span><span class=\"param\"><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.llm_templates.SavePeftModelCallback.on_save", "modulename": "autotransformers.llm_templates", "qualname": "SavePeftModelCallback.on_save", "kind": "function", "doc": "<p>Triggered when saving the model during training. Calls <code>save_model</code> and returns control.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>args : TrainerArguments\n    Arguments for the trainer.\nstate : TrainerState\n    Current trainer state.\ncontrol : str\n    Control string for trainer callback.\nkwargs : dict\n    Additional keyword arguments.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>str\n    Control string.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"n\">state</span>, </span><span class=\"param\"><span class=\"n\">control</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.llm_templates.SavePeftModelCallback.on_train_end", "modulename": "autotransformers.llm_templates", "qualname": "SavePeftModelCallback.on_train_end", "kind": "function", "doc": "<p>Triggered at the end of training. Creates a 'completed' file and calls <code>save_model</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>args : TrainerArguments\n    Arguments for the trainer.\nstate : TrainerState\n    Current trainer state.\ncontrol : str\n    Control string for trainer callback.\nkwargs : dict\n    Additional keyword arguments.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>None</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"n\">state</span>, </span><span class=\"param\"><span class=\"n\">control</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.metrics", "modulename": "autotransformers.metrics", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.metrics.metric_sum", "modulename": "autotransformers.metrics", "qualname": "metric_sum", "kind": "variable", "doc": "<p></p>\n", "default_value": "EvaluationModule(name: &quot;rouge&quot;, module_type: &quot;metric&quot;, features: [{&#x27;predictions&#x27;: Value(dtype=&#x27;string&#x27;, id=&#x27;sequence&#x27;), &#x27;references&#x27;: Sequence(feature=Value(dtype=&#x27;string&#x27;, id=&#x27;sequence&#x27;), length=-1, id=None)}, {&#x27;predictions&#x27;: Value(dtype=&#x27;string&#x27;, id=&#x27;sequence&#x27;), &#x27;references&#x27;: Value(dtype=&#x27;string&#x27;, id=&#x27;sequence&#x27;)}], usage: &quot;&quot;&quot;\nCalculates average rouge scores for a list of hypotheses and references\nArgs:\n    predictions: list of predictions to score. Each prediction\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\n    rouge_types: A list of rouge types to calculate.\n        Valid names:\n        `&quot;rouge{n}&quot;` (e.g. `&quot;rouge1&quot;`, `&quot;rouge2&quot;`) where: {n} is the n-gram based scoring,\n        `&quot;rougeL&quot;`: Longest common subsequence based scoring.\n        `&quot;rougeLsum&quot;`: rougeLsum splits text using `&quot;\n&quot;`.\n        See details in https://github.com/huggingface/datasets/issues/617\n    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n    use_aggregator: Return aggregates if this is set to True\nReturns:\n    rouge1: rouge_1 (f1),\n    rouge2: rouge_2 (f1),\n    rougeL: rouge_l (f1),\n    rougeLsum: rouge_lsum (f1)\nExamples:\n\n    &gt;&gt;&gt; rouge = evaluate.load(&#x27;rouge&#x27;)\n    &gt;&gt;&gt; predictions = [&quot;hello there&quot;, &quot;general kenobi&quot;]\n    &gt;&gt;&gt; references = [&quot;hello there&quot;, &quot;general kenobi&quot;]\n    &gt;&gt;&gt; results = rouge.compute(predictions=predictions, references=references)\n    &gt;&gt;&gt; print(results)\n    {&#x27;rouge1&#x27;: 1.0, &#x27;rouge2&#x27;: 1.0, &#x27;rougeL&#x27;: 1.0, &#x27;rougeLsum&#x27;: 1.0}\n&quot;&quot;&quot;, stored examples: 0)"}, {"fullname": "autotransformers.metrics.metric_seqeval", "modulename": "autotransformers.metrics", "qualname": "metric_seqeval", "kind": "variable", "doc": "<p></p>\n", "default_value": "EvaluationModule(name: &quot;seqeval&quot;, module_type: &quot;metric&quot;, features: {&#x27;predictions&#x27;: Sequence(feature=Value(dtype=&#x27;string&#x27;, id=&#x27;label&#x27;), length=-1, id=&#x27;sequence&#x27;), &#x27;references&#x27;: Sequence(feature=Value(dtype=&#x27;string&#x27;, id=&#x27;label&#x27;), length=-1, id=&#x27;sequence&#x27;)}, usage: &quot;&quot;&quot;\nProduces labelling scores along with its sufficient statistics\nfrom a source against one or more references.\n\nArgs:\n    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n    references: List of List of reference labels (Ground truth (correct) target values)\n    suffix: True if the IOB prefix is after type, False otherwise. default: False\n    scheme: Specify target tagging scheme. Should be one of [&quot;IOB1&quot;, &quot;IOB2&quot;, &quot;IOE1&quot;, &quot;IOE2&quot;, &quot;IOBES&quot;, &quot;BILOU&quot;].\n        default: None\n    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n        If you want to only count exact matches, pass mode=&quot;strict&quot;. default: None.\n    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n        &quot;warn&quot;. &quot;warn&quot; acts as 0, but the warning is raised.\n\nReturns:\n    &#x27;scores&#x27;: dict. Summary of the scores for overall and per type\n        Overall:\n            &#x27;accuracy&#x27;: accuracy,\n            &#x27;precision&#x27;: precision,\n            &#x27;recall&#x27;: recall,\n            &#x27;f1&#x27;: F1 score, also known as balanced F-score or F-measure,\n        Per type:\n            &#x27;precision&#x27;: precision,\n            &#x27;recall&#x27;: recall,\n            &#x27;f1&#x27;: F1 score, also known as balanced F-score or F-measure\nExamples:\n\n    &gt;&gt;&gt; predictions = [[&#x27;O&#x27;, &#x27;O&#x27;, &#x27;B-MISC&#x27;, &#x27;I-MISC&#x27;, &#x27;I-MISC&#x27;, &#x27;I-MISC&#x27;, &#x27;O&#x27;], [&#x27;B-PER&#x27;, &#x27;I-PER&#x27;, &#x27;O&#x27;]]\n    &gt;&gt;&gt; references = [[&#x27;O&#x27;, &#x27;O&#x27;, &#x27;O&#x27;, &#x27;B-MISC&#x27;, &#x27;I-MISC&#x27;, &#x27;I-MISC&#x27;, &#x27;O&#x27;], [&#x27;B-PER&#x27;, &#x27;I-PER&#x27;, &#x27;O&#x27;]]\n    &gt;&gt;&gt; seqeval = evaluate.load(&quot;seqeval&quot;)\n    &gt;&gt;&gt; results = seqeval.compute(predictions=predictions, references=references)\n    &gt;&gt;&gt; print(list(results.keys()))\n    [&#x27;MISC&#x27;, &#x27;PER&#x27;, &#x27;overall_precision&#x27;, &#x27;overall_recall&#x27;, &#x27;overall_f1&#x27;, &#x27;overall_accuracy&#x27;]\n    &gt;&gt;&gt; print(results[&quot;overall_f1&quot;])\n    0.5\n    &gt;&gt;&gt; print(results[&quot;PER&quot;][&quot;f1&quot;])\n    1.0\n&quot;&quot;&quot;, stored examples: 0)"}, {"fullname": "autotransformers.metrics.compute_metrics_classification", "modulename": "autotransformers.metrics", "qualname": "compute_metrics_classification", "kind": "function", "doc": "<p>Compute metrics for classification (multi-class or binary) tasks.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>pred: transformers.EvalPrediction\n    Prediction as output by transformers.Trainer\ntokenizer: transformers.Tokenizer\n    Tokenizer from huggingface.\nid2tag: Dict\n    Dictionary mapping label ids to label names.\nadditional_metrics: List\n    List with additional metrics to compute.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>metrics: Dict\n    Dictionary with metrics. For information regarding the exact metrics\n    received in it, see the documentation for sklearn.metrics.classification_report.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">id2tag</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">additional_metrics</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.metrics.compute_metrics_multilabel", "modulename": "autotransformers.metrics", "qualname": "compute_metrics_multilabel", "kind": "function", "doc": "<p>Compute the metrics for a multilabel task.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>pred: transformers.EvalPrediction\n    Prediction as output by transformers.Trainer\ntokenizer: transformers.Tokenizer\n    Tokenizer from huggingface.\nid2tag: Dict\n    Dictionary mapping label ids to label names.\nadditional_metrics: List\n    List with additional metrics to compute.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>best_metrics: Dict\n    Dictionary with best metrics, after trying different thresholds.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">id2tag</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">additional_metrics</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.metrics.compute_metrics_ner", "modulename": "autotransformers.metrics", "qualname": "compute_metrics_ner", "kind": "function", "doc": "<p>Compute metrics for ner.</p>\n\n<p>Use seqeval metric from HF Evaluate. Get the predicted label for each instance,\nthen skip padded tokens and finally use seqeval metric, which takes into account\nfull entities, not individual tokens, when computing the metrics.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>p: transformers.EvalPrediction\n    Instance of EvalPrediction from transformers.\ntokenizer: transformers.Tokenizer\n    Tokenizer from huggingface.\nid2tag: Dict\n    Dictionary mapping label ids to label names.\nadditional_metrics: List\n    List with additional metrics to compute.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Metrics\n    Complete dictionary with all computed metrics on eval data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">p</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">id2tag</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">additional_metrics</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.metrics.compute_metrics_summarization", "modulename": "autotransformers.metrics", "qualname": "compute_metrics_summarization", "kind": "function", "doc": "<p>Compute metrics for summarization tasks, by using rouge metrics in datasets library.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>eval_pred: transformers.EvalPrediction\n    Prediction as output by transformers.Trainer\ntokenizer:\n    Tokenizer from huggingface.\nid2tag: Dict\n    Dictionary mapping label ids to label names.\nadditional_metrics: List\n    List with additional metrics to compute.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>metrics: Dict\n    Dictionary with relevant metrics for summarization.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">eval_pred</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span>, </span><span class=\"param\"><span class=\"n\">id2tag</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">additional_metrics</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.metrics_plotter", "modulename": "autotransformers.metrics_plotter", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter", "kind": "class", "doc": "<p>Tool for plotting the results of the models trained.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>metrics_dir: str\n    Directory name with metrics.\nmodel_names: List\n    List with the names of the models.\ndataset_to_task_map: Dict\n    Dictionary that maps dataset names to tasks. Can be built with the list of DatasetConfigs.\nremove_strs: List\n    List of strings to remove from filename.\nmetric_field: str\n    Name of the field with the objective metric.\ncompute_models_average: bool\n    Whether to compute the models' average score to plot.</p>\n"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.__init__", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">metrics_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_names</span><span class=\"p\">:</span> <span class=\"n\">List</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_to_task_map</span><span class=\"p\">:</span> <span class=\"n\">Dict</span>,</span><span class=\"param\">\t<span class=\"n\">remove_strs</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>,</span><span class=\"param\">\t<span class=\"n\">metric_field</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;f1-score&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">compute_models_average</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">modify_metric_func</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">modified_metric_fieldname</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.metrics_dir", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.metrics_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.model_names", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.model_names", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.dataset_to_task_map", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.dataset_to_task_map", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.remove_strs", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.remove_strs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.metric_field", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.metric_field", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.compute_models_average", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.compute_models_average", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.modify_metric_func", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.modify_metric_func", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.modified_metric_fieldname", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.modified_metric_fieldname", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.plot_metrics", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.plot_metrics", "kind": "function", "doc": "<p>Plot the metrics as a barplot.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.metrics_plotter.ResultsPlotter.read_metrics", "modulename": "autotransformers.metrics_plotter", "qualname": "ResultsPlotter.read_metrics", "kind": "function", "doc": "<p>Read the metrics in the self.metrics_dir directory, creating a dataset with the data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.model_config", "modulename": "autotransformers.model_config", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.model_config.ModelConfig", "modulename": "autotransformers.model_config", "qualname": "ModelConfig", "kind": "class", "doc": "<p>Configure a model to use inside the AutoTrainer class.</p>\n\n<p>With this we determine every choice related to the model\nsuch as the original name, the name to save the model with, the hyperparameter space, and a long etc.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>name: str\n    Name of the model, either in the HF hub or a path to the local directory where it is stored.\nsave_name: str\n    Alias for the model, used for saving it.\nhp_space\n    The hyperparameter space for hyperparameter search with optuna. Must be a function receiving a trial and returning a dictionary with the corresponding suggest_categorical and float fields.\ndropout_vals: List\n    Dropout values to try.\ncustom_config_class: transformers.PretrainedConfig\n    Custom configuration for a model. Useful for training ensembles of transformers.\ncustom_model_class: transformers.PreTrainedModel\n    Custom model. None by default. Only used for ensemble models and other strange creatures of Nature.\npartial_custom_tok_func_call: Any\n    Partial call for a tokenization function, with all necessary parameters passed to it.\nmax_length_summary: int\n    Max length of the summaries. Useful for summarization datasets.\nmin_length_summary : int\n    Min length of the summaries. Useful for summarization datasets.\nno_repeat_ngram_size: int\n    Number of n-grams to don't repeat when doing summarization.\nearly_stopping_summarization: bool\n    Whether to have early stopping when doing summarization tasks.\nlength_penalty: float\n    Length penalty for summarization tasks.\nnum_beams: int\n    Number of beams in beam search for summarization tasks.\ndropout_field_name: str\n    Name for the dropout field in the pooler layer.\nn_trials : int\n    Number of trials (trainings) to carry out with this model.\nrandom_init_trials: int\n    Argument for optuna sampler, to control number of initial trials to run randomly.\ntrainer_cls_summarization: Any\n    Class for the trainer. Useful when it is desired to override the default trainer cls for summarization.\nmodel_cls_summarization: Any\n    Class for the trainer. Useful when it is desired to override the default trainer cls for summarization.\ncustom_tokenization_func: Any\n    Custom tokenization function for processing texts. When the user does not want to use the default tokenization function for the task at hand, one can create a custom tokenization function. This function must receive samples from a dataset, a tokenizer and a dataset config.\nonly_test: bool\n    Whether to only test, not train (for already trained models).\ntest_batch_size: int\n    Batch size for test; only used when doing only testing.\noverwrite_training_args: Dict\n    Arguments to overwrite the default arguments for the trainer, for example to change the optimizer for this concrete model.\nsave_dir: str\n    The directory to save the trained model.\npush_to_hub: bool\n    Whether to push the best model to the hub.\nadditional_params_tokenizer: Dict\n    Additional arguments to pass to the tokenizer.\nresume_from_checkpoint: bool\n    Whether to resume from checkpoint to continue training.\nconfig_problem_type: str\n    The type of the problem, for loss fct.\ncustom_trainer_cls: Any\n    Custom trainer class to override the current one.\ndo_nothing: bool\n    Whether to do nothing or not. If true, will not train nor predict.\ncustom_params_config_model: Dict\n    Dictionary with custom parameters for loading AutoConfig.\ngeneration_params: Dict\n    Parameters for generative tasks, for the generate call.\nhf_hub_username: str\n    Username in HF Hub, to push models to hub.\ncustom_results_getter: Any\n    Custom class to get test results after training.\nquantization_config: BitsAndBytesConfig\n    Configuration for quantizing models.\ncustom_params_model: Dict\n    Dictionary with custom parameters for initializing the model.\nmodel_init_wrap_cls: Any\n    Wrap class with a <code>__call__</code> method for the <code>model_init</code> function, useful for example for preparing quantized models, etc.\npeft_config: Any\n    Configuration class from Peft. For example, <code>LoraConfig</code>.\nfunc_modify_tokenizer: Any\n    Function to modify the behavior of the tokenizer, for example to set the pad token id (for decoder-only models this is typically mandatory) or for changing the padding size.</p>\n\n<h2 id=\"examples\">Examples</h2>\n\n<p>With the following lines you can create a ModelConfig for bert-base-cased model.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">autotransformers</span> <span class=\"kn\">import</span> <span class=\"n\">ModelConfig</span>\n</code></pre>\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">autotransformers.default_param_spaces</span> <span class=\"kn\">import</span> <span class=\"n\">hp_space_base</span>\n</code></pre>\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">model_config</span> <span class=\"o\">=</span> <span class=\"n\">ModelConfig</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;bert-base-cased&#39;</span><span class=\"p\">,</span> <span class=\"n\">save_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;bert&#39;</span><span class=\"p\">,</span> <span class=\"n\">hp_space</span><span class=\"o\">=</span><span class=\"n\">hp_space_base</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n"}, {"fullname": "autotransformers.model_config.ModelConfig.__init__", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">save_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">hp_space</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dropout_vals</span><span class=\"p\">:</span> <span class=\"n\">List</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">factory</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">custom_config_class</span><span class=\"p\">:</span> <span class=\"n\">transformers</span><span class=\"o\">.</span><span class=\"n\">configuration_utils</span><span class=\"o\">.</span><span class=\"n\">PretrainedConfig</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">custom_model_class</span><span class=\"p\">:</span> <span class=\"n\">transformers</span><span class=\"o\">.</span><span class=\"n\">modeling_utils</span><span class=\"o\">.</span><span class=\"n\">PreTrainedModel</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">custom_tokenization_func</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_length_summary</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>,</span><span class=\"param\">\t<span class=\"n\">min_length_summary</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">no_repeat_ngram_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping_summarization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">length_penalty</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">2.0</span>,</span><span class=\"param\">\t<span class=\"n\">num_beams</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">dropout_field_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;cls_dropout&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">n_trials</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>,</span><span class=\"param\">\t<span class=\"n\">random_init_trials</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">trainer_cls_summarization</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_cls_summarization</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">only_test</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">test_batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">overwrite_training_args</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;.&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">push_to_hub</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">additional_params_tokenizer</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">resume_from_checkpoint</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">custom_trainer_cls</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">do_nothing</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">custom_params_config_model</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">generation_params</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">hf_hub_username</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">custom_results_getter</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">quantization_config</span><span class=\"p\">:</span> <span class=\"n\">transformers</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">quantization_config</span><span class=\"o\">.</span><span class=\"n\">BitsAndBytesConfig</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">custom_params_model</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_init_wrap_cls</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">peft_config</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">func_modify_tokenizer</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">optuna_sampler</span><span class=\"p\">:</span> <span class=\"n\">Any</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">neftune_noise_alpha</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">alignment_config</span><span class=\"p\">:</span> <span class=\"n\">Dict</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "autotransformers.model_config.ModelConfig.name", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "autotransformers.model_config.ModelConfig.save_name", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.save_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "autotransformers.model_config.ModelConfig.hp_space", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.hp_space", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.dropout_vals", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.dropout_vals", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List"}, {"fullname": "autotransformers.model_config.ModelConfig.custom_config_class", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.custom_config_class", "kind": "variable", "doc": "<p></p>\n", "annotation": ": transformers.configuration_utils.PretrainedConfig", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.custom_model_class", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.custom_model_class", "kind": "variable", "doc": "<p></p>\n", "annotation": ": transformers.modeling_utils.PreTrainedModel", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.custom_tokenization_func", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.custom_tokenization_func", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.max_length_summary", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.max_length_summary", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "128"}, {"fullname": "autotransformers.model_config.ModelConfig.min_length_summary", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.min_length_summary", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "10"}, {"fullname": "autotransformers.model_config.ModelConfig.no_repeat_ngram_size", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.no_repeat_ngram_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "3"}, {"fullname": "autotransformers.model_config.ModelConfig.early_stopping_summarization", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.early_stopping_summarization", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "autotransformers.model_config.ModelConfig.length_penalty", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.length_penalty", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "2.0"}, {"fullname": "autotransformers.model_config.ModelConfig.num_beams", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.num_beams", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "1"}, {"fullname": "autotransformers.model_config.ModelConfig.dropout_field_name", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.dropout_field_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;cls_dropout&#x27;"}, {"fullname": "autotransformers.model_config.ModelConfig.n_trials", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.n_trials", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "20"}, {"fullname": "autotransformers.model_config.ModelConfig.random_init_trials", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.random_init_trials", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "10"}, {"fullname": "autotransformers.model_config.ModelConfig.trainer_cls_summarization", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.trainer_cls_summarization", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.model_cls_summarization", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.model_cls_summarization", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.only_test", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.only_test", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.model_config.ModelConfig.test_batch_size", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.test_batch_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "32"}, {"fullname": "autotransformers.model_config.ModelConfig.overwrite_training_args", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.overwrite_training_args", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.save_dir", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.save_dir", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;.&#x27;"}, {"fullname": "autotransformers.model_config.ModelConfig.push_to_hub", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.push_to_hub", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.model_config.ModelConfig.additional_params_tokenizer", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.additional_params_tokenizer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.resume_from_checkpoint", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.resume_from_checkpoint", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.model_config.ModelConfig.custom_trainer_cls", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.custom_trainer_cls", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.do_nothing", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.do_nothing", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "autotransformers.model_config.ModelConfig.custom_params_config_model", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.custom_params_config_model", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.generation_params", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.generation_params", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.hf_hub_username", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.hf_hub_username", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.custom_results_getter", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.custom_results_getter", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.quantization_config", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.quantization_config", "kind": "variable", "doc": "<p></p>\n", "annotation": ": transformers.utils.quantization_config.BitsAndBytesConfig", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.custom_params_model", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.custom_params_model", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.model_init_wrap_cls", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.model_init_wrap_cls", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.peft_config", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.peft_config", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.func_modify_tokenizer", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.func_modify_tokenizer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.optuna_sampler", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.optuna_sampler", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.neftune_noise_alpha", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.neftune_noise_alpha", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "None"}, {"fullname": "autotransformers.model_config.ModelConfig.alignment_config", "modulename": "autotransformers.model_config", "qualname": "ModelConfig.alignment_config", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict", "default_value": "None"}, {"fullname": "autotransformers.results_getter", "modulename": "autotransformers.results_getter", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.results_getter.ResultsGetter", "modulename": "autotransformers.results_getter", "qualname": "ResultsGetter", "kind": "class", "doc": "<p>Retrieve results on the test set for different tasks (seq2seq, different forms of classification, NER, QA...).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset_config: autotransformers.DatasetConfig\n    Configuration for the dataset.\nmodel_config: autotransformers.ModelConfig\n    Configuration for the model.\ncompute_metrics_func: Any\n    Function to compute metrics.</p>\n"}, {"fullname": "autotransformers.results_getter.ResultsGetter.__init__", "modulename": "autotransformers.results_getter", "qualname": "ResultsGetter.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">autotransformers</span><span class=\"o\">.</span><span class=\"n\">dataset_config</span><span class=\"o\">.</span><span class=\"n\">DatasetConfig</span>,</span><span class=\"param\">\t<span class=\"n\">model_config</span><span class=\"p\">:</span> <span class=\"n\">autotransformers</span><span class=\"o\">.</span><span class=\"n\">model_config</span><span class=\"o\">.</span><span class=\"n\">ModelConfig</span>,</span><span class=\"param\">\t<span class=\"n\">compute_metrics_func</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span>)</span>"}, {"fullname": "autotransformers.results_getter.ResultsGetter.dataset_config", "modulename": "autotransformers.results_getter", "qualname": "ResultsGetter.dataset_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.results_getter.ResultsGetter.model_config", "modulename": "autotransformers.results_getter", "qualname": "ResultsGetter.model_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.results_getter.ResultsGetter.compute_metrics_func", "modulename": "autotransformers.results_getter", "qualname": "ResultsGetter.compute_metrics_func", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.results_getter.ResultsGetter.get_test_results_summarization", "modulename": "autotransformers.results_getter", "qualname": "ResultsGetter.get_test_results_summarization", "kind": "function", "doc": "<p>Compute and get the results in test for summarization tasks.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>test_dataset: datasets.Dataset\n    Test dataset.\ntrainer: transformers.Trainer\n    HF's transformers trainer.\ncompute_metrics_func: Any\n    Function to compute metrics.\nmodel_config: autotransformers.ModelConfig\n    Configuration for the model.\nadditional_metrics: List\n    List with additional metrics to compute.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>metrics: Dict\n    Dictionary with metrics for the summarization task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">test_dataset</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span>,</span><span class=\"param\">\t<span class=\"n\">compute_metrics_func</span>,</span><span class=\"param\">\t<span class=\"n\">additional_metrics</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.results_getter.ResultsGetter.general_get_test_results", "modulename": "autotransformers.results_getter", "qualname": "ResultsGetter.general_get_test_results", "kind": "function", "doc": "<p>Compute metrics in general for every NLU task except for QA.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>test_dataset: datasets.Dataset\n    Dataset  on any task except for QA.\ntrainer: transformers.Trainer\n    Trainer trained on a dataset that is not a QA dataset.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>metrics: Dict\n    Metrics for the test dataset.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">test_dataset</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span>,</span><span class=\"param\">\t<span class=\"n\">compute_metrics_func</span>,</span><span class=\"param\">\t<span class=\"n\">additional_metrics</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.results_getter.ResultsGetter.get_test_results_qa", "modulename": "autotransformers.results_getter", "qualname": "ResultsGetter.get_test_results_qa", "kind": "function", "doc": "<p>Compute metrics on test for QA datasets.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>test_dataset: datasets.Dataset\n    QA dataset.\ntrainer: transformers.Trainer\n    Trainer trained on QA dataset.\nsquad_v2: bool\n    Whether the dataset is in squad v2 format or not.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>metrics: Dict\n    Metrics for the test dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">test_dataset</span>, </span><span class=\"param\"><span class=\"n\">trainer</span>, </span><span class=\"param\"><span class=\"n\">squad_v2</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">additional_metrics</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.results_getter.ResultsGetter.prepare_validation_features_squad", "modulename": "autotransformers.results_getter", "qualname": "ResultsGetter.prepare_validation_features_squad", "kind": "function", "doc": "<p>Process features for validating on squad-like datasets.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>examples: datasets.Dataset\n    Samples from datasets.Dataset.\ntokenizer: tokenizers.Tokenizer\n    Instance of hf's tokenizer.\npad_on_right: bool\n    Whether or not to pad the samples on the right side. True for most models.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tokenized_examples:\n    Tokenized samples.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">examples</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span>, </span><span class=\"param\"><span class=\"n\">pad_on_right</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.results_getter.ResultsGetter.postprocess_qa_predictions", "modulename": "autotransformers.results_getter", "qualname": "ResultsGetter.postprocess_qa_predictions", "kind": "function", "doc": "<p>Process raw predictions of a QA model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>examples: datasets.Dataset\n    Samples from datasets.Dataset.\nfeatures:\n    Validation features as processed by prepare_validation_features_squad.\nraw_predictions:\n    Predictions by trainer.\ntokenizer: tokenizers.Tokenizer\n    Instance of hf's tokenizer.\nn_best_size: int\n    Number of best answers to get (maximum).\nmax_answer_length: int\n    Maximum answer length in number of characters. Answer longer than this are not even considered.\nsquad_v2: bool\n    Whether the dataset is in squad v2 format or not.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>predictions: collections.OrderedDict\n    An ordered dict with the predictions formatted so that we can compute metrics easily.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">examples</span>,</span><span class=\"param\">\t<span class=\"n\">features</span>,</span><span class=\"param\">\t<span class=\"n\">raw_predictions</span>,</span><span class=\"param\">\t<span class=\"n\">tokenizer</span>,</span><span class=\"param\">\t<span class=\"n\">n_best_size</span><span class=\"o\">=</span><span class=\"mi\">20</span>,</span><span class=\"param\">\t<span class=\"n\">max_answer_length</span><span class=\"o\">=</span><span class=\"mi\">30</span>,</span><span class=\"param\">\t<span class=\"n\">squad_v2</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">min_score</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.skip_mix", "modulename": "autotransformers.skip_mix", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.skip_mix.SkipMix", "modulename": "autotransformers.skip_mix", "qualname": "SkipMix", "kind": "class", "doc": "<p>Simple class to skip mix of dataset and model.</p>\n\n<p>Two properties: dataset to skip and model to skip.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset_name: str\n    Name of the dataset, as in alias parameter of DatasetConfig.\nmodel_name: str\n    Name of the model, as in save_name parameter of ModelConfig.</p>\n"}, {"fullname": "autotransformers.skip_mix.SkipMix.__init__", "modulename": "autotransformers.skip_mix", "qualname": "SkipMix.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "autotransformers.skip_mix.SkipMix.dataset_name", "modulename": "autotransformers.skip_mix", "qualname": "SkipMix.dataset_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.skip_mix.SkipMix.model_name", "modulename": "autotransformers.skip_mix", "qualname": "SkipMix.model_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "autotransformers.tokenization_functions", "modulename": "autotransformers.tokenization_functions", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.tokenization_functions.tokenize_classification", "modulename": "autotransformers.tokenization_functions", "qualname": "tokenize_classification", "kind": "function", "doc": "<p>Tokenize classification datasets.</p>\n\n<p>Given a dataset, a tokenizer and a dataset configuration, returns\nthe tokenized dataset.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>examples: datasets.Dataset\n    Samples from datasets.Dataset.\ntokenizer: tokenizers.Tokenizer\n    Instance of hf's tokenizer.\ndataset_config: benchmarker.DatasetConfig\n    Instance of a Dataset Config.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tokenized:\n    Tokenized samples.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">examples</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span>, </span><span class=\"param\"><span class=\"n\">dataset_config</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.tokenization_functions.tokenize_ner", "modulename": "autotransformers.tokenization_functions", "qualname": "tokenize_ner", "kind": "function", "doc": "<p>Tokenize a dataset or dataset split.</p>\n\n<p>This function is intended to be used inside the map method for the Dataset.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>examples: datasets.Dataset\n    Samples from datasets.Dataset.\ntokenizer: tokenizers.Tokenizer\n    Instance of hf's tokenizer.\ndataset_config: benchmarker.DatasetConfig\n    Instance of a Dataset Config.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tokenized:\n    Tokenized samples.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">examples</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span>, </span><span class=\"param\"><span class=\"n\">dataset_config</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.tokenization_functions.tokenize_squad", "modulename": "autotransformers.tokenization_functions", "qualname": "tokenize_squad", "kind": "function", "doc": "<p>Tokenize samples of squad-like datasets, on batches.</p>\n\n<p>It differentiates between BPE tokenizers and others\nas there are errors in these ones if they are processed in the conventional way.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>examples: datasets.Dataset\n    Samples from datasets.Dataset.\ntokenizer: tokenizers.Tokenizer\n    Instance of hf's tokenizer.\npad_on_right: bool\n    Whether or not to pad the samples on the right side. True for most models.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tokenized_examples:\n    Tokenized samples.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">examples</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span>, </span><span class=\"param\"><span class=\"n\">dataset_config</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">pad_on_right</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.tokenization_functions.tokenize_summarization", "modulename": "autotransformers.tokenization_functions", "qualname": "tokenize_summarization", "kind": "function", "doc": "<p>Tokenization function for summarization tasks.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>examples: datasets.Dataset\n    Samples from datasets.Dataset.\ntokenizer: tokenizers.Tokenizer\n    Instance of hf's tokenizer.\ndataset_config: autotransformers.DatasetConfig\n    Instance of a Dataset Config.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>examples: datasets.Dataset\n    Tokenized samples with all necessary fields.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">examples</span>, </span><span class=\"param\"><span class=\"n\">tokenizer</span>, </span><span class=\"param\"><span class=\"n\">dataset_config</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.tokenization_functions.tokenize_one_element_chatbot", "modulename": "autotransformers.tokenization_functions", "qualname": "tokenize_one_element_chatbot", "kind": "function", "doc": "<p>Tokenize one element for a chatbot, by adding the eos token if requested and ensuring the label for it is not -100.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tokenizer: PreTrainedTokenizer\n    Tokenizer to tokenize text data.\ntext: str\n    The text to tokenize and process.\nadd_eos_token: bool\n    Whether to add the EOS (end of sentece) token at the end of input ids.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result: Dict\n    Dict as returned by the tokenizer, with <code>input_ids</code>, <code>attention_mask</code> and <code>labels</code> keys.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokenizer</span><span class=\"p\">:</span> <span class=\"n\">transformers</span><span class=\"o\">.</span><span class=\"n\">tokenization_utils</span><span class=\"o\">.</span><span class=\"n\">PreTrainedTokenizer</span>,</span><span class=\"param\">\t<span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">add_eos_token</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.tokenization_functions.tokenize_chatbot", "modulename": "autotransformers.tokenization_functions", "qualname": "tokenize_chatbot", "kind": "function", "doc": "<p>Tokenize inputs for chatbot or instruction tuning.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>samples: Dict\n    Dataset samples to process.\ntokenizer: PreTrainedTokenizer\n    Tokenizer to tokenize text data.\nadd_eos_token: bool\n    Whether to add eos token at the end or not.\ntrain_on_inputs: bool\n    Whether to train on inputs. If false, will mask out labels from inputs with -100 so that the model only learns on outputs.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>samples: Dict\n    Processed samples with tokenized data.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">Dict</span>,</span><span class=\"param\">\t<span class=\"n\">tokenizer</span><span class=\"p\">:</span> <span class=\"n\">transformers</span><span class=\"o\">.</span><span class=\"n\">tokenization_utils</span><span class=\"o\">.</span><span class=\"n\">PreTrainedTokenizer</span>,</span><span class=\"param\">\t<span class=\"n\">add_eos_token</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">autotransformers</span><span class=\"o\">.</span><span class=\"n\">dataset_config</span><span class=\"o\">.</span><span class=\"n\">DatasetConfig</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.utils", "modulename": "autotransformers.utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "autotransformers.utils.joinpaths", "modulename": "autotransformers.utils", "qualname": "joinpaths", "kind": "function", "doc": "<p>Join all paths passed as args.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">paths</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.utils.filter_empty", "modulename": "autotransformers.utils", "qualname": "filter_empty", "kind": "function", "doc": "<p>Remove empty characters and spaces from list.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>string: str\n    String to filter.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>result: bool\n    Whether string is not in the empty characters list.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">string_list</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.utils.dict_to_list", "modulename": "autotransformers.utils", "qualname": "dict_to_list", "kind": "function", "doc": "<p>Transform a dictionary of entities in the default format.</p>\n\n<p>With start and end characters for each entity, into lists of words and\nlabels, having one label per word. This is useful for NER tasks\nwhen we usually have this format (ent_label, start_char, end_char)\nand we need to have 2 equally-sized lists of words and labels for\npassing them to the tokenizer.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>example:\n    Sample of huggingface Dataset, with an entities field containing\n    the entities in the format mentioned above.\nnulltoken: Union[str, int]\n    Default token for the \"no-entities\". Usually O is used for this,\n    which is the default value.\nentities_field: str\n    Name of the field which contains entities in (ent_label, start_char, end_char)\n    format. Usually \"entities\" is used for this, which is the default value.\nsentence_field: str\n    Name of the field which contains the sentence. Usually \"sentence\" is used\n    for this, which is the default value.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>example:\n    Sample of huggingface dataset with 2 new fields: token list and\n    label list.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">example</span>,</span><span class=\"param\">\t<span class=\"n\">nulltoken</span><span class=\"o\">=</span><span class=\"s1\">&#39;O&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">entities_field</span><span class=\"o\">=</span><span class=\"s1\">&#39;entities&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">sentence_field</span><span class=\"o\">=</span><span class=\"s1\">&#39;sentence&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.utils.get_windowed_match_context_answer", "modulename": "autotransformers.utils", "qualname": "get_windowed_match_context_answer", "kind": "function", "doc": "<p>Find the best possible match for an answer in the context.</p>\n\n<p>Useful for translated QA datasets, where we don't have exact translations\nof the answers and they do not exist in the context anymore. This could also\nhappen because of encodings, or other reasons, which cause that the answer\ndoes not start at the string index that appears in the dataset.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>context: str\n    Context where we want to find the answer.\nanswer: str\n    Answer that we want to find in the context.\nmaxrange: int\n    Maximum size of the windows for matching, in number of words.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>beg: int\n    Beginning character index of the answer.\nend: int\n    Ending character index for tha answer.\nnew_answer: str\n    Answer found in the context.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">context</span>, </span><span class=\"param\"><span class=\"n\">answer</span>, </span><span class=\"param\"><span class=\"n\">maxrange</span><span class=\"o\">=</span><span class=\"mi\">100</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.utils.match_questions_multiple_answers", "modulename": "autotransformers.utils", "qualname": "match_questions_multiple_answers", "kind": "function", "doc": "<p>Check if any of the given answers for a question coincides with our answer.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>formatted_predictions: List\n    List with the predictions.\nreferences: List\n    All references with real answers for the questions. Possibly more than one\n    answer per question, which we need to unify previously with the same id.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>final_references: List\n    Final references for the questions, so that if we get right questions with\n    more than one possible answers, it counts as a right guess.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">formatted_predictions</span>, </span><span class=\"param\"><span class=\"n\">references</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.utils.get_tags", "modulename": "autotransformers.utils", "qualname": "get_tags", "kind": "function", "doc": "<p>Get the list of unique tags for a dataset.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>dataset: datasets.DatasetDict\n    Dataset to tokenize.\ndataset_config: autotransformers.DatasetConfig\n    Dataset configuration.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tags: List\n    List of unique labels for the dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset</span>, </span><span class=\"param\"><span class=\"n\">dataset_config</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.utils.chunks", "modulename": "autotransformers.utils", "qualname": "chunks", "kind": "function", "doc": "<p>Split a list into n-sized chunks.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>lst: List\n    List containing any type of elements.\nn: int\n    Size of the chunks</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Chunks:\n    Generates n-sized chunks.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">lst</span>, </span><span class=\"param\"><span class=\"n\">n</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "autotransformers.utils.fix_eval_results_dict", "modulename": "autotransformers.utils", "qualname": "fix_eval_results_dict", "kind": "function", "doc": "<p>Remove test or eval set prefix from names in the metrics dict.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>metrics: Dict\n    Dictionary with metrics.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>new_metrics: Dict\n    Fixed metrics dictionary.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">metrics</span><span class=\"p\">:</span> <span class=\"nb\">dict</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();