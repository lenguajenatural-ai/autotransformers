{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avacaondata/nlpboost/blob/main/notebooks/extractive_qa/train_sqac.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Question Answering in Spanish: SQAC\n",
    "\n",
    "In this tutorial we will see how we can train multiple Spanish models on a QA dataset in that language: SQAC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the needed modules or, if you are running this notebook in Google colab, please uncomment the cell below and run it before importing, in order to install `nlpboost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpboost import AutoTrainer, ModelConfig, DatasetConfig, ResultsPlotter\n",
    "from transformers import EarlyStoppingCallback\n",
    "from nlpboost.default_param_spaces import hp_space_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the dataset\n",
    "\n",
    "The next step is to define the fixed train args, which will be the `transformers.TrainingArguments` passed to `transformers.Trainer` inside `nlpboost.AutoTrainer`. For a full list of arguments check [TrainingArguments documentation](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.TrainingArguments). `DatasetConfig` expects these arguments in dictionary format.\n",
    "\n",
    "To save time, we set `max_steps` to 1; in a real setting we would need to define these arguments differently. However, that is out of scope for this tutorial. To learn how to work with Transformers, and how to configure the training arguments, please check Huggingface Course on NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_train_args = {\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"num_train_epochs\": 10,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"save_total_limit\": 2,\n",
    "        \"seed\": 69,\n",
    "        \"fp16\": True,\n",
    "        \"dataloader_num_workers\": 8,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"adam_epsilon\": 1e-6,\n",
    "        \"adam_beta1\": 0.9,\n",
    "        \"adam_beta2\": 0.999,\n",
    "        \"max_steps\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define some common args for the dataset. In this case we minimize the loss, as for QA no compute metrics function is used during training. We use the loss to choose the best model and then compute metrics over the test set, which is not a straightforward process (that is the reason for not computing metrics in-training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args_dataset = {\n",
    "        \"seed\": 44,\n",
    "        \"direction_optimize\": \"minimize\",\n",
    "        \"metric_optimize\": \"eval_loss\",\n",
    "        \"retrain_at_end\": False,\n",
    "        \"callbacks\": [EarlyStoppingCallback(1, 0.00001)],\n",
    "        \"fixed_training_args\": fixed_train_args\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define arguments specific of SQAC. In this case, the text field and the label col are not used, so we just set them to two string columns of the dataset. In QA tasks, `nlpboost` assumes the dataset is in SQUAD format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqac_config = default_args_dataset.copy()\n",
    "sqac_config.update(\n",
    "    {\n",
    "        \"dataset_name\": \"sqac\",\n",
    "        \"alias\": \"sqac\",\n",
    "        \"task\": \"qa\",\n",
    "        \"text_field\": \"context\",\n",
    "        \"hf_load_kwargs\": {\"path\": \"PlanTL-GOB-ES/SQAC\"},\n",
    "        \"label_col\": \"question\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqac_config = DatasetConfig(**sqac_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Models\n",
    "\n",
    "We will configure three Spanish models. As you see, we only need to define the `name`, which is the path to the model (either in HF Hub or locally), `save_name` which is an arbitrary name for the model, the hyperparameter space and the number of trials. There are more parameters, which you can check in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertin_config = ModelConfig(\n",
    "        name=\"bertin-project/bertin-roberta-base-spanish\",\n",
    "        save_name=\"bertin\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1,\n",
    ")\n",
    "beto_config = ModelConfig(\n",
    "        name=\"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "        save_name=\"beto\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1,\n",
    ")\n",
    "albert_config = ModelConfig(\n",
    "        name=\"CenIA/albert-tiny-spanish\",\n",
    "        save_name=\"albert\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train! \n",
    "\n",
    "We can train now these three models on the SQAC dataset and see how well they perform (remember, if you really want to train them please remove the max steps to 1 in the fixed training arguments and the number of trials to 1 in the model configs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotrainer = AutoTrainer(\n",
    "        model_configs=[bertin_config, beto_config, albert_config],\n",
    "        dataset_configs=[sqac_config],\n",
    "        metrics_dir=\"spanish_qa_metrics\",\n",
    "        metrics_cleaner=\"spanish_qa_cleaner_metrics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = autotrainer()\n",
    "print(experiment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "\n",
    "As in other tutorials, we can now plot the results with ResultsPlotter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = ResultsPlotter(\n",
    "    metrics_dir=autotrainer.metrics_dir,\n",
    "    model_names=[model_config.save_name for model_config in autotrainer.model_configs],\n",
    "    dataset_to_task_map={dataset_config.alias: dataset_config.task for dataset_config in autotrainer.dataset_configs},\n",
    ")\n",
    "ax = plotter.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('largesum': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac692fd94dcfa608ba1aabbfbe7d5467f50ca857b57fe228a116df0c8b5b792"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
