{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avacaondata/nlpboost/blob/main/notebooks/NER/train_spanish_ner.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition in Spanish\n",
    "\n",
    "In this tutorial, we will see how we can train Spanish models for two different NER tasks: `conll2002`, which has the typical tags PER, LOC, ORG, and `ehealth_kd`, which labels correspond to entities in the biomedical domain. Additionally, these datasets do not come in the same format, so we will see how we can add a preprocessing function to `DatasetConfig` so that we can train with NER datasets in many different formats.\n",
    "\n",
    "We first import the needed modules or, if you are running this notebook in Google colab, please uncomment the cell below and run it before importing, in order to install `nlpboost`.\n",
    "\n",
    "We import `DatasetConfig`, the class that configures how datasets are managed inside `AutoTrainer`. We also need `ModelConfig` to define the models to train, and `ResultsPlotter` to plot the experiment results. The function `dict_to_list` will help us with `ehealth_kd` dataset, which has a field with texts, and a field with entities in a list of dictionaries. However, we need two equally-sized lists for each data instance: the list of tokens and the list of the labels of those tokens. `dict_to_list` will perform that preprocessing for us.\n",
    "Additionally, we import the default hyperparameter space for base-sized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/avacaondata/nlpboost.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpboost import AutoTrainer, DatasetConfig, ModelConfig, dict_to_list, ResultsPlotter\n",
    "from transformers import EarlyStoppingCallback\n",
    "from nlpboost.default_param_spaces import hp_space_base\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the dataset\n",
    "\n",
    "The next step is to define the fixed train args, which will be the `transformers.TrainingArguments` passed to `transformers.Trainer` inside `nlpboost.AutoTrainer`. For a full list of arguments check [TrainingArguments documentation](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.TrainingArguments). `DatasetConfig` expects these arguments in dictionary format.\n",
    "\n",
    "To save time, we set `max_steps` to 1; in a real setting we would need to define these arguments differently. However, that is out of scope for this tutorial. To learn how to work with Transformers, and how to configure the training arguments, please check Huggingface Course on NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_train_args = {\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"num_train_epochs\": 10,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"logging_strategy\": \"steps\",\n",
    "        \"eval_steps\": 1,\n",
    "        \"save_steps\": 1,\n",
    "        \"logging_steps\": 1,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_total_limit\": 2,\n",
    "        \"seed\": 69,\n",
    "        \"fp16\": True,\n",
    "        \"no_cuda\": False,\n",
    "        \"dataloader_num_workers\": 2,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"adam_epsilon\": 1e-6,\n",
    "        \"adam_beta1\": 0.9,\n",
    "        \"adam_beta2\": 0.999,\n",
    "        \"max_steps\": 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the default arguments that all NER datasets will share. That common config includes the random seed, the direction to optimize, the metric, callbacks and fixed training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args_dataset = {\n",
    "        \"seed\": 44,\n",
    "        \"direction_optimize\": \"maximize\",\n",
    "        \"metric_optimize\": \"eval_f1-score\",\n",
    "        \"retrain_at_end\": False,\n",
    "        \"callbacks\": [EarlyStoppingCallback(1, 0.00001)],\n",
    "        \"fixed_training_args\": fixed_train_args\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not start building conll2002 configuration. As this dataset already comes with a list of tokens and a list of labels for each row, we can directly use these two columns as text field and label col respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2002_config = default_args_dataset.copy()\n",
    "conll2002_config.update(\n",
    "    {\n",
    "        \"dataset_name\": \"conll2002\",\n",
    "        \"alias\": \"conll2002\",\n",
    "        \"task\": \"ner\",\n",
    "        \"text_field\": \"tokens\",\n",
    "        \"hf_load_kwargs\": {\"path\": \"conll2002\", \"name\": \"es\"},\n",
    "        \"label_col\": \"ner_tags\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2002_config = DatasetConfig(**conll2002_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to prepare the configuration of ehealth_kd. As you see, in this case we use a `pre_func` (`dict_to_list`) to preprocess the dataset. As that function will return a list of labels called label_list, that is the name we use for `label_col` in the config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehealth_config = default_args_dataset.copy()\n",
    "\n",
    "ehealth_config.update(\n",
    "    {\n",
    "        \"dataset_name\": \"ehealth_kd\",\n",
    "        \"alias\": \"ehealth\",\n",
    "        \"task\": \"ner\",\n",
    "        \"text_field\": \"token_list\",\n",
    "        \"hf_load_kwargs\": {\"path\": \"ehealth_kd\"},\n",
    "        \"label_col\": \"label_list\",\n",
    "        \"pre_func\": partial(dict_to_list, nulltoken=100)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehealth_config = DatasetConfig(**ehealth_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_configs = [\n",
    "        conll2002_config,\n",
    "        ehealth_config\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Models\n",
    "\n",
    "We will configure three Spanish models. As you see, we only need to define the `name`, which is the path to the model (either in HF Hub or locally), `save_name` which is an arbitrary name for the model, the hyperparameter space and the number of trials. There are more parameters, which you can check in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertin_config = ModelConfig(\n",
    "        name=\"bertin-project/bertin-roberta-base-spanish\",\n",
    "        save_name=\"bertin\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1,\n",
    ")\n",
    "beto_config = ModelConfig(\n",
    "        name=\"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "        save_name=\"beto\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1,\n",
    ")\n",
    "albert_config = ModelConfig(\n",
    "        name=\"CenIA/albert-tiny-spanish\",\n",
    "        save_name=\"albert\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Train\n",
    "\n",
    "Now we can configure `AutoTrainer` with the dataset configs and model configs defined above, and we are ready to train just by calling the autotrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotrainer = AutoTrainer(\n",
    "        model_configs=[bertin_config, beto_config, albert_config],\n",
    "        dataset_configs=dataset_configs,\n",
    "        metrics_dir=\"metrics_spanish_ner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = autotrainer()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Results\n",
    "\n",
    "Once the models have trained, we might want to see a comparison of their performance. With `ResultsPlotter` we can easily do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = ResultsPlotter(\n",
    "        metrics_dir=autotrainer.metrics_dir,\n",
    "        model_names=[model_config.save_name for model_config in autotrainer.model_configs],\n",
    "        dataset_to_task_map={dataset_config.alias: dataset_config.task for dataset_config in autotrainer.dataset_configs},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plotter.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('largesum': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac692fd94dcfa608ba1aabbfbe7d5467f50ca857b57fe228a116df0c8b5b792"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
