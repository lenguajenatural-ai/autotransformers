{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lenguajenatural-ai/autotransformers/blob/master/notebooks/classification/train_emotion_classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Classification with autotransformers\n",
    "\n",
    "In this notebook we will see how we can use autotransformers to train a model for text classification, in the context of emotion detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the needed modules or, if you are running this notebook in Google colab, please uncomment the cell below and run it before importing, in order to install `autotransformers`.\n",
    "\n",
    "We import `DatasetConfig`, the class that configures how datasets are managed inside `AutoTrainer`. We also need `ModelConfig` to define the models to train, and `ResultsPlotter` to plot the experiment results.\n",
    "Additionally, we import the default hyperparameter space for base-sized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/lenguajenatural-ai/autotransformers.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autotransformers import DatasetConfig, ModelConfig, AutoTrainer, ResultsPlotter\n",
    "from autotransformers.default_param_spaces import hp_space_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the fixed train args, which will be the `transformers.TrainingArguments` passed to `transformers.Trainer` inside `autotransformers.AutoTrainer`. For a full list of arguments check [TrainingArguments documentation](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments). `DatasetConfig` expects these arguments in dictionary format.\n",
    "\n",
    "To save time, we set `max_steps` to 1; in a real setting we would need to define these arguments differently. However, that is out of scope for this tutorial. To learn how to work with Transformers, and how to configure the training arguments, please check Huggingface Course on NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_train_args = {\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"num_train_epochs\": 10,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"logging_strategy\": \"steps\",\n",
    "        \"eval_steps\": 1,\n",
    "        \"save_steps\": 1,\n",
    "        \"logging_steps\": 1,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_total_limit\": 2,\n",
    "        \"seed\": 69,\n",
    "        \"fp16\": False,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"max_steps\": 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, there are some parameters that will be common for any dataset we may want to train on of this type (classification). We call this dictionary `default_args_dataset`. This dictionary configures the direction of the optimization process (for hyperparameter search), the metric to optimize, whether to retrain models at end or not, and the `fixed_train_args`, which are common to all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args_dataset = {\n",
    "        \"seed\": 44,\n",
    "        \"direction_optimize\": \"maximize\",\n",
    "        \"metric_optimize\": \"eval_f1-score\",\n",
    "        \"retrain_at_end\": False,\n",
    "        \"fixed_training_args\": fixed_train_args\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create a configuration which has all of that, but with some additional parameters. We will call it `tweet_eval_config`, as it is the `DatasetConfig` of tweeteval dataset. Next we define the dataset name, the alias (in this case the same as the dataset name), the task (classification in this case), the text field (field in the dataset where source texts are), the label col, which is the name of the field in the dataset containing the labels, and finally `hf_load_kwargs` is a dictionary with the keyword arguments needed to load the dataset from HF's datasets \n",
    "(`load_dataset(**hf_load_kwargs)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_eval_config = default_args_dataset.copy()\n",
    "tweet_eval_config.update(\n",
    "    {\n",
    "        \"dataset_name\": \"tweeteval\",\n",
    "        \"alias\": \"tweeteval\",\n",
    "        \"task\": \"classification\",\n",
    "        \"text_field\": \"text\",\n",
    "        \"label_col\": \"label\",\n",
    "        \"hf_load_kwargs\": {\"path\": \"tweet_eval\", \"name\": \"emotion\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have the full dataset configuration ready, we can easily create the `DatasetConfig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_eval_config = DatasetConfig(**tweet_eval_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Models\n",
    "\n",
    "Once we have the dataset configuration ready, we just need to configure the model. In this case, we are going to use BERTIN model, which is a Spanish RoBERTa model. We use the default `hp_space_base` as the hyperparameter space, and set number of trials to 1. For a real setting, please set the number of trials to at least 20 to obtain realiable results. Otherwise the hyperparameter tuning will serve for nothing.\n",
    "\n",
    "Additionally, we will create another 2 model configs, to show how we can later compare their results etc.\n",
    "\n",
    "**Note that we are using Spanish models for an English task. As we are not actually trying to train realistic good performing models for this task this does not matter, as this notebook is for learning purposes solely. However, please make sure you choose models that fit your tasks when using `autotransformers` for real projects.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertin_config = ModelConfig(\n",
    "        name=\"bertin-project/bertin-roberta-base-spanish\",\n",
    "        save_name=\"bertin\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1,\n",
    ")\n",
    "beto_config = ModelConfig(\n",
    "        name=\"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "        save_name=\"beto\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1,\n",
    ")\n",
    "albert_config = ModelConfig(\n",
    "        name=\"CenIA/albert-tiny-spanish\",\n",
    "        save_name=\"albert\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AutoTrainer\n",
    "\n",
    "We can now create `AutoTrainer`. For that, we will use the model configs and the dataset config we have just created. We will additionally define a metrics dir, where metrics will be saved after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotrainer = AutoTrainer(\n",
    "        model_configs=[bertin_config, beto_config, albert_config],\n",
    "        dataset_configs=[tweet_eval_config],\n",
    "        metrics_dir=\"tweeteval_metrics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!\n",
    "\n",
    "Now we can train the selected models on the tweeteval dataset by calling the `AutoTrainer` object. We also print out the results, which are obviously awful in this case because we trained for one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = autotrainer()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Results\n",
    "\n",
    "Once the models have trained, we might want to see a comparison of their performance. `ResultsPlotter` can be helpful in this respect, as we see in the next cells. It automatically adds an average row, which shows the average performance of the models over the different datasets. This is useful when comparing multiple models over multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = ResultsPlotter(\n",
    "        metrics_dir=autotrainer.metrics_dir,\n",
    "        model_names=[model_config.save_name for model_config in autotrainer.model_configs],\n",
    "        dataset_to_task_map={dataset_config.alias: dataset_config.task for dataset_config in autotrainer.dataset_configs},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plotter.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('largesum': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac692fd94dcfa608ba1aabbfbe7d5467f50ca857b57fe228a116df0c8b5b792"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
