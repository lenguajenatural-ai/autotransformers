{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/lenguajenatural-ai/autotransformers/blob/master/notebooks/chatbot_instructions/train_instructional_chatbot.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autotransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Instructional Chatbots in Spanish\n",
    "\n",
    "In this tutorial, we'll explore how to train instructional chatbots in Spanish using the [somos-clean-alpaca](https://huggingface.co/datasets/somosnlp/somos-clean-alpaca-es) dataset. This dataset provides a rich collection of conversational and instructional interactions in Spanish, making it an ideal resource for developing chatbots capable of understanding and executing specific instructions. We'll leverage the `autotransformers` library to streamline the training process, applying advanced techniques such as LoRA and quantization for efficient model adaptation and performance. Whether you're looking to enhance an existing chatbot or build a new one from scratch, this guide will equip you with the knowledge and tools needed to succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "\n",
    "Before we begin, it's essential to import the necessary libraries that will be used throughout the tutorial. These libraries provide the foundational tools required for loading datasets, configuring models, and training. Below is a brief overview of each import and its role in our project:\n",
    "\n",
    "- `from autotransformers import AutoTrainer, DatasetConfig, ModelConfig`: Imports the `AutoTrainer` class for orchestrating the training process, and `DatasetConfig` and `ModelConfig` for configuring the dataset and model parameters, respectively, within the `autotransformers` library.\n",
    "\n",
    "- `from autotransformers.llm_templates import QLoraWrapperModelInit, modify_tokenizer, qlora_config, SavePeftModelCallback`: These imports from the `autotransformers` library's large language model (LLM) templates module include utilities for initializing models with LoRA wrappers, modifying tokenizers to fit our task, configuring quantization (QLoRA), and implementing a callback to save PEFT (Post-training Efficiency Fine-tuning) models.\n",
    "\n",
    "- `from functools import partial`: The `partial` function from the `functools` module is used to partially apply functions, allowing us to pre-specify some arguments of a function, which is particularly useful for customizing our tokenizer modification function.\n",
    "\n",
    "- `from peft import LoraConfig`: Imports the `LoraConfig` class from the `peft` library, which is used to specify configurations for LoRA (Low-Rank Adaptation), an efficient method for adapting pre-trained models to new tasks with minimal computational overhead.\n",
    "\n",
    "- `from datasets import load_dataset`: From the `datasets` library, we import the `load_dataset` function, which is used to load and preprocess data from a wide range of datasets available in the Hugging Face Datasets repository, including our target dataset `somos-clean-alpaca-es`.\n",
    "\n",
    "Ensure all these libraries are installed in your environment before proceeding with the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autotransformers import AutoTrainer, DatasetConfig, ModelConfig\n",
    "from autotransformers.llm_templates import instructions_to_chat, NEFTuneTrainer, QLoraWrapperModelInit, modify_tokenizer, qlora_config, SavePeftModelCallback\n",
    "from functools import partial\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Chat Template\n",
    "\n",
    "To correctly format the conversations for training, we define a chat template using Jinja2 templating syntax. This template iterates through each message in a conversation, categorizing and formatting them based on their role:\n",
    "\n",
    "- **User Messages**: Wrapped with `<user>` tags to clearly indicate messages from the user. These are the instructions or queries directed at the chatbot.\n",
    "\n",
    "- **System Messages**: Enclosed within `<system>` tags, followed by line breaks for readability. These messages might include system-generated instructions or context that guides the chatbot's responses.\n",
    "\n",
    "- **Assistant Responses**: Placed between the conversation, after `</user>` tags and marked with `</assistant>` tags at the end, along with the end-of-sentence (EOS) token. These are the chatbot's replies or actions taken in response to the user's message, at each utterance or intervention in the conversation.\n",
    "\n",
    "- **Input Data**: Marked with `<input>` tags to distinguish any additional input or contextual information provided to the chatbot.\n",
    "\n",
    "This structured format is crucial for the model to understand the different components of a conversation, enabling it to generate appropriate responses based on the role of each message.\n",
    "\n",
    "Typically, a conversation will start with the system message, then have an input containing additional context for the assistant, and then turns of user-assistant, which can be one or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_TEMPLATE = \"\"\"{% for message in messages %}\n",
    "    {% if message['role'] == 'user' %}\n",
    "        {{'<user> ' + message['content'].strip() + ' </user>' }}\n",
    "    {% elif message['role'] == 'system' %}\n",
    "        {{'<system>\\\\n' + message['content'].strip() + '\\\\n</system>\\\\n\\\\n' }}\n",
    "    {% elif message['role'] == 'assistant' %}\n",
    "        {{ message['content'].strip() + ' </assistant>' + eos_token }}\n",
    "    {% elif message['role'] == 'input' %}\n",
    "        {{'<input> ' + message['content'] + ' </input>' }}\n",
    "    {% endif %}\n",
    "{% endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "The dataset preparation phase is crucial for structuring the data in a way that's conducive to training a chatbot. We first load the dataset from the hub and then utilize a custom function, `process_alpaca`, to transform each sample from the `somos-clean-alpaca` dataset into a format that mirrors a real conversation flow involving a system message, user input, and assistant response.\n",
    "\n",
    "### The `process_alpaca` Function\n",
    "\n",
    "`process_alpaca` takes a dictionary representing a single dataset sample and restructures it by categorizing and ordering messages based on their role in a conversation:\n",
    "\n",
    "- It starts by adding a **system message** that sets the context for the chatbot as an assistant designed to follow user instructions.\n",
    "- If present, **input data** is added next to provide additional context or information needed to fulfill the user's request.\n",
    "- The **user's instruction** is then added, followed by the **assistant's output**, which is the response to the user's request.\n",
    "\n",
    "This restructuring results in a `messages` list within the sample dictionary, containing all conversation elements in their logical order.\n",
    "\n",
    "### Applying the Transformation\n",
    "\n",
    "To apply this transformation across the entire dataset:\n",
    "\n",
    "- We use the `.map` method with `process_alpaca` as the mapping function, setting `batched=False` to process samples individually and `num_proc=4` to parallelize the operation, enhancing efficiency.\n",
    "- Columns not part of the `messages` structure are removed to streamline the dataset.\n",
    "\n",
    "Finally, the dataset is split into training and test sets with a 20% test size, ensuring that we can evaluate our chatbot's performance on unseen data. This split is achieved using the `train_test_split` method, providing a solid foundation for training and validating the chatbot model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = load_dataset(\"somosnlp/somos-clean-alpaca-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_alpaca(sample: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Processes a single sample from the alpaca dataset to structure it for chatbot training.\n",
    "\n",
    "    This function transforms the dataset sample into a format suitable for training,\n",
    "    where each message is categorized by its role in the conversation (system, input, user, assistant).\n",
    "    It initializes the conversation with a system message, then conditionally adds an input message,\n",
    "    follows with the user's instruction, and finally, the assistant's output based on the provided inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : dict\n",
    "        A dictionary representing a single sample from the dataset. It must contain\n",
    "        keys corresponding to input and output components of the conversation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A modified dictionary with a 'messages' key that contains a list of ordered messages,\n",
    "        each annotated with its role in the conversation.\n",
    "    \"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente que resuelve las instrucciones del usuario. Si se proporciona contexto adicional, utiliza esa información para completar la instrucción.\"}\n",
    "    ]\n",
    "    inp_ = sample[\"inputs\"][\"2-input\"] \n",
    "    if inp_ is not None and inp_ != \"\":\n",
    "        chat.append(\n",
    "            {\"role\": \"input\", \"content\": inp_}\n",
    "        )\n",
    "    chat.extend(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": sample[\"inputs\"][\"1-instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"inputs\"][\"3-output\"]}\n",
    "        ]\n",
    "    )\n",
    "    sample[\"messages\"] = chat\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente, podemos usar directamente la función `instructions_to_chat` de `llm_templates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = alpaca.map(\n",
    "    partial(\n",
    "        instructions_to_chat,\n",
    "        input_field=\"1-instruction\",\n",
    "        context_field=\"2-input\",\n",
    "        output_field=\"3-output\",\n",
    "        nested_field=\"inputs\"\n",
    "    ),\n",
    "    batched=False,\n",
    "    num_proc=4,\n",
    "    remove_columns=[col for col in alpaca[\"train\"].column_names if col != \"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = alpaca[\"train\"].train_test_split(0.2, seed=203984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Dataset for AutoTransformers\n",
    "\n",
    "To ensure our instructional chatbot model trains efficiently and effectively, we meticulously configure our dataset using the `autotransformers` library's `DatasetConfig`. This step is essential for tailoring the training process to our specific needs, including hyperparameter settings, dataset particulars, and training strategies.\n",
    "\n",
    "### Setting Up Training Arguments\n",
    "\n",
    "A set of fixed training arguments (`fixed_train_args`) is defined to control various aspects of the training process:\n",
    "\n",
    "- **Batch sizes** for both training and evaluation are set to 1, indicating that samples are processed individually. This can be particularly useful for large models or when GPU memory is limited.\n",
    "- **Gradient accumulation** is used with 16 steps, allowing us to effectively simulate a larger batch size and stabilize training without exceeding memory limits.\n",
    "- A **warmup ratio** of 0.03 gradually increases the learning rate at the beginning of training to prevent the model from converging too quickly to a suboptimal solution.\n",
    "- **Learning rate**, **weight decay**, and other optimization settings are carefully chosen to balance model learning speed and quality.\n",
    "- **Evaluation and saving strategies** are configured to periodically check the model's performance and save checkpoints, enabling monitoring and continuation of training from the last saved state.\n",
    "\n",
    "### Crafting the Dataset Configuration\n",
    "\n",
    "The `alpaca_config` dictionary encompasses all necessary information for dataset preparation and integration:\n",
    "\n",
    "- **Dataset details** such as name, task type, and specific columns to use for text and labels ensure that the model trains on the correct data format.\n",
    "- **Training parameters** are included via the `fixed_training_args` dictionary.\n",
    "- **Callback classes**, such as `SavePeftModelCallback`, automate important steps like model saving during training.\n",
    "- **Process optimizations** like setting a seed for reproducibility, specifying the optimization direction and metric, and enabling partial splits for validation set creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_train_args = {\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"bf16\": True,\n",
    "    \"logging_steps\": 50,\n",
    "    \"lr_scheduler_type\": \"constant\",\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"eval_steps\": 200,\n",
    "    \"save_steps\": 50,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"logging_first_step\": True,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \"optim\": \"paged_adamw_32bit\",\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"group_by_length\": False,\n",
    "    \"save_total_limit\": 50,\n",
    "    \"adam_beta2\": 0.999\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_config = {\n",
    "        \"seed\": 9834,\n",
    "        \"direction_optimize\": \"minimize\",\n",
    "        \"metric_optimize\": \"eval_loss\",\n",
    "        \"callbacks\": [SavePeftModelCallback],\n",
    "        \"fixed_training_args\": fixed_train_args,\n",
    "        \"dataset_name\": \"alpaca\",\n",
    "        \"alias\": \"alpaca\",\n",
    "        \"retrain_at_end\": False,\n",
    "        \"task\": \"chatbot\",\n",
    "        \"text_field\": \"messages\",\n",
    "        \"label_col\": \"messages\",\n",
    "        \"num_proc\": 4,\n",
    "        \"loaded_dataset\": alpaca,\n",
    "        \"partial_split\": True, # to create a validation split.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_config = DatasetConfig(**alpaca_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "In the \"Model Configuration\" section, we outline how to set up the model configurations using `autotransformers`, focusing on integrating LoRA (Low-Rank Adaptation) for model adaptation and applying quantization for efficiency. These steps are crucial for tailoring the model to our specific task and environment, ensuring optimal performance and resource utilization.\n",
    "\n",
    "### LoRA Configuration\n",
    "\n",
    "The `LoraConfig` object is instantiated with parameters tailored to enhance model adaptability while maintaining efficiency:\n",
    "\n",
    "- **r (rank)** and **lora_alpha** are set to adjust the capacity and learning rate multiplier for LoRA layers, balancing between model flexibility and overfitting risk.\n",
    "- **target_modules** specifies which parts of the model to apply LoRA. In this case, \"all-linear\" modules are targeted for adaptation, offering a broad enhancement over the model's capabilities.\n",
    "- **lora_dropout** is adjusted based on the model size, ensuring that regularization is appropriately scaled.\n",
    "- **bias** configuration is set to \"none\", indicating that no additional bias terms are used in the LoRA adaptation layers.\n",
    "- The **task_type** is specified as \"CAUSAL_LM\" to indicate the causal language modeling task, aligning with the instructional chatbot's nature.\n",
    "\n",
    "### GEMMA Model Configuration\n",
    "\n",
    "The `ModelConfig` for the GEMMA model includes several key parameters and customizations:\n",
    "\n",
    "- **Model Name**: Specifies the pre-trained model to be adapted, \"google/gemma-2b-it\" in this case.\n",
    "- **Save Name and Directory**: Defines the naming convention and location for saving the fine-tuned model.\n",
    "- **Custom Parameters**: Includes model-specific settings, such as enabling trust in remote code and configuring device mapping for training.\n",
    "- **Model Initialization Wrapper**: `QLoraWrapperModelInit` is used to integrate the QLoRA quantization framework with the LoRA-configured model, optimizing for both adaptability and efficiency.\n",
    "- **Quantization and PEFT Configurations**: These are applied via the `quantization_config` and `peft_config` parameters, ensuring that the model benefits from both LoRA adaptations and efficient post-training quantization.\n",
    "- **Tokenizer Modification**: A partial function is used to customize the tokenizer, adjusting sequence length, adding special tokens, and incorporating the chat template designed for our conversational context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        target_modules=\"all-linear\",  # \"query_key_value\" # \"Wqkv\"\n",
    "        lora_dropout=0.05,  # 0.1 for <13B models, 0.05 otherwise.\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_config = ModelConfig(\n",
    "    name=\"google/gemma-2b-it\",\n",
    "    save_name=\"gemma_2b_alpaca\",\n",
    "    save_dir=\"./gemma_2b_alpaca\",\n",
    "    custom_params_model={\"trust_remote_code\": True, \"device_map\": {\"\": 0}},\n",
    "    model_init_wrap_cls=QLoraWrapperModelInit,\n",
    "    quantization_config=qlora_config,\n",
    "    peft_config=lora_config,\n",
    "    neftune_noise_alpha=10,\n",
    "    custom_trainer_cls=NEFTuneTrainer,\n",
    "    func_modify_tokenizer=partial(\n",
    "        modify_tokenizer,\n",
    "        new_model_seq_length=4096, # lower the maximum seq length to 4096 instead of 8192 to fit in google colab GPUs.\n",
    "        add_special_tokens={\"pad_token\": \"[PAD]\"}, # add pad token.\n",
    "        chat_template=CHAT_TEMPLATE # add the new chat template including the system and input roles.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Train\n",
    "\n",
    "With our dataset and model configurations in place, we're now ready to initiate the training process. This is where the `AutoTrainer` class from the `autotransformers` library comes into play, orchestrating the entire training operation based on the specifications we've provided.\n",
    "\n",
    "### Setting Up the AutoTrainer\n",
    "\n",
    "The `AutoTrainer` is a comprehensive class designed to streamline the training of machine learning models, especially tailored for large language models. It accepts several parameters to control the training process:\n",
    "\n",
    "- **Model Configurations**: A list of `ModelConfig` objects, each defining the settings and customizations for a model. For our instructional chatbot, we include the configuration for the GEMMA model adapted with LoRA and quantization.\n",
    "- **Dataset Configurations**: Similar to model configurations, these are specified using `DatasetConfig` objects. We pass the configuration for our pre-processed and structured `alpaca` dataset, ensuring it's utilized effectively during training.\n",
    "- **Metrics Directory**: Specifies the directory where training metrics will be stored, allowing for performance monitoring and evaluation.\n",
    "- **Hyperparameter Search Mode**: Set to \"fixed\" in our case, indicating that we're not exploring different hyperparameters but rather training with a predetermined set.\n",
    "- **Clean**: A boolean flag to clean any previous runs' data, ensuring a fresh start for each training session.\n",
    "- **Metrics Cleaner**: Specifies the utility for handling temporary metrics data, keeping our metrics directory tidy and focused on significant results.\n",
    "- **Use Auth Token**: Enables the use of an authentication token, necessary for accessing certain models or datasets that may have access restrictions.\n",
    "\n",
    "### Initiating the Training\n",
    "\n",
    "With the `AutoTrainer` configured, we proceed to call its execution method. This step starts the training process, leveraging the configurations we've meticulously set up. The process involves:\n",
    "\n",
    "- Automatically loading and preparing the dataset according to our `DatasetConfig`.\n",
    "- Adapting and fine-tuning the model based on the `ModelConfig`, including any specified LoRA or quantization enhancements.\n",
    "- Regularly evaluating the model's performance using the provided validation set, allowing us to monitor its effectiveness in real-time.\n",
    "- Saving model checkpoints and training metrics, enabling both introspection of the training process and the resumption of training from the last saved state.\n",
    "\n",
    "Upon completion, the training results, including performance metrics and model checkpoints, are made available for analysis and deployment. This step marks the culmination of our instructional chatbot's preparation, rendering it ready for testing and eventually, deployment in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotrainer = AutoTrainer(\n",
    "    model_configs=[gemma_config],\n",
    "    dataset_configs=[alpaca_config],\n",
    "    metrics_dir=\"./metrics_alpaca\",\n",
    "    hp_search_mode=\"fixed\",\n",
    "    clean=True,\n",
    "    metrics_cleaner=\"tmp_metrics_cleaner\",\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = autotrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatnatural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
