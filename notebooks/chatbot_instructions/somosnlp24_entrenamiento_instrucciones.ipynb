{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/lenguajenatural-ai/autotransformers/blob/master/notebooks/chatbot_instructions/somosnlp24_entrenamiento_instrucciones.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon SomosNLP 2024: Entrenamiento de LLMs\n",
    "\n",
    "En este tutorial veremos cómo entrenar LLMs para instrucciones / chat con las herramientas de HuggingFace. En la siguiente parte del notebook veremos cómo hacer esto mismo con [autotransformers](https://github.com/lenguajenatural-ai/autotransformers), añadiendo el entrenamiento con NEFTune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero de todo instalamos la librería `autotransformers` que ya nos va a traer directamente el resto de dependencias que necesitamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autotransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de Librerías\n",
    "\n",
    "Este bloque de código se encarga de importar todas las librerías necesarias para el funcionamiento del script. Se importan herramientas para la manipulación de modelos de aprendizaje automático como `torch` y `transformers`, así como librerías específicas para la preparación y configuración de modelos (`peft`), carga y procesamiento de conjuntos de datos (`datasets`), y una librería especial (`trl`) para el entrenamiento de modelos de lenguaje mediante técnicas de fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig, TrainingArguments,  AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "from peft.tuners.lora import LoraLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando la plantilla de chat\n",
    "\n",
    "En esta sección, se crea una plantilla para formatear los mensajes de chat durante el entrenamiento. La plantilla utiliza sintaxis específica para identificar y organizar los roles de los participantes en la conversación (usuario, sistema, asistente, entrada), permitiendo que el modelo comprenda y genere respuestas adecuadas dentro del contexto establecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chat template\n",
    "CHAT_TEMPLATE = \"\"\"{% for message in messages %}\n",
    "    {% if message['role'] == 'user' %}\n",
    "        {{'<user> ' + message['content'].strip() + ' </user>' }}\n",
    "    {% elif message['role'] == 'system' %}\n",
    "        {{'<system>\\\\n' + message['content'].strip() + '\\\\n</system>\\\\n\\\\n' }}\n",
    "    {% elif message['role'] == 'assistant' %}\n",
    "        {{ message['content'].strip() + ' </assistant>' + eos_token }}\n",
    "    {% elif message['role'] == 'input' %}\n",
    "        {{'<input> ' + message['content'] + ' </input>' }}\n",
    "    {% endif %}\n",
    "{% endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del dataset y preprocesado\n",
    "\n",
    "Se carga un dataset específico llamado `somosnlp/somos-clean-alpaca-es` usando la librería `datasets`. Posteriormente, se define y aplica una función de preprocesado (`process_alpaca`) que estructura cada muestra del dataset en un formato adecuado para entrenar chatbots, etiquetando cada mensaje con su respectivo rol en la conversación. Finalmente, el dataset procesado se divide en conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = load_dataset(\"somosnlp/somos-clean-alpaca-es\")\n",
    "\n",
    "def process_alpaca(sample: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Processes a single sample from the alpaca dataset to structure it for chatbot training.\n",
    "\n",
    "    This function transforms the dataset sample into a format suitable for training,\n",
    "    where each message is categorized by its role in the conversation (system, input, user, assistant).\n",
    "    It initializes the conversation with a system message, then conditionally adds an input message,\n",
    "    follows with the user's instruction, and finally, the assistant's output based on the provided inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : dict\n",
    "        A dictionary representing a single sample from the dataset. It must contain\n",
    "        keys corresponding to input and output components of the conversation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A modified dictionary with a 'messages' key that contains a list of ordered messages,\n",
    "        each annotated with its role in the conversation.\n",
    "    \"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente que resuelve las instrucciones del usuario. Si se proporciona contexto adicional, utiliza esa información para completar la instrucción.\"}\n",
    "    ]\n",
    "    inp_ = sample[\"inputs\"][\"2-input\"] \n",
    "    if inp_ is not None and inp_ != \"\":\n",
    "        chat.append(\n",
    "            {\"role\": \"input\", \"content\": inp_}\n",
    "        )\n",
    "    chat.extend(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": sample[\"inputs\"][\"1-instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"inputs\"][\"3-output\"]}\n",
    "        ]\n",
    "    )\n",
    "    sample[\"messages\"] = chat\n",
    "    return sample\n",
    "\n",
    "alpaca = alpaca.map(\n",
    "    process_alpaca,\n",
    "    batched=False,\n",
    "    num_proc=4,\n",
    "    remove_columns=[col for col in alpaca[\"train\"].column_names if col != \"messages\"])\n",
    "\n",
    "alpaca = alpaca[\"train\"].train_test_split(0.2, seed=203984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de los argumentos de entrenamiento\n",
    "\n",
    "Se configuran los argumentos de entrenamiento utilizando la clase `TrainingArguments` de la librería `transformers`. Estos argumentos incluyen configuraciones importantes como el tamaño del batch, la tasa de aprendizaje, el tipo de optimizador, y varios otros parámetros que influencian directamente en el rendimiento y la eficiencia del entrenamiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma_2b_alpaca\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    warmup_ratio=0.03,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    weight_decay=0.001,\n",
    "    eval_steps=200,\n",
    "    save_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    logging_first_step=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    max_grad_norm=0.3,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=False,\n",
    "    save_total_limit=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del tokenizador\n",
    "\n",
    "Se carga un tokenizador preentrenado correspondiente al modelo `google/gemma-2b` usando la librería `transformers`. Además, se configura el tokenizador con la plantilla de chat creada anteriormente y se ajustan parámetros específicos como el token de relleno y la longitud máxima de secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2b\"\n",
    "max_seq_length = 4096\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "tokenizer.model_max_length = max_seq_length\n",
    "tokenizer.chat_template=CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de formateo del chat\n",
    "\n",
    "Esta función toma las muestras del dataset y las procesa aplicando la plantilla de chat configurada previamente. El objetivo es tokenizar las entradas para que el modelo pueda entender y generar respuestas durante el entrenamiento y la evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat(\n",
    "    samples: dict,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Tokenize inputs for chatbot or instruction tuning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples: Dict\n",
    "        Dataset samples to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    samples: Dict\n",
    "        Processed samples with tokenized data.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for i in range(len(samples[\"messages\"])):\n",
    "        full_text = tokenizer.apply_chat_template(\n",
    "            samples[\"messages\"][i], tokenize=False\n",
    "        )\n",
    "        texts.append(full_text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del modelo\n",
    "\n",
    "Se configura y carga el modelo de lenguaje causal para entrenamiento con cuantización y ajustes específicos para mejorar el rendimiento y reducir el consumo de memoria. Se utiliza una configuración específica para LoRA (Low-Rank Adaptation) y QLoRA (Quantized LoRA), ajustando parámetros como el rango y la tasa de dropout, y se prepara el modelo para el entrenamiento con estos ajustes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64, # NOTE: Al usar rslora podemos subir el rango con mejoras en el rendimiento.\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\", # NOTE: En QLoRA entrenamos todas las capas lineales del modelo.\n",
    "    lora_dropout=0.10,  # 0.1 for <13B models, 0.05 otherwise.\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_rslora=True # NOTE: flag para usar QLoRA.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qlora_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # NOTE: Lo cargamos en 4bits.\n",
    "    bnb_4bit_use_double_quant=True, # NOTE: Usamos la doble cuantización de QLoRA para ahorrar aún más espacio.\n",
    "    bnb_4bit_quant_type=\"nf4\", # NOTE: Usamos NormalFloat 4bits ya que según el paper de QLoRA funciona mejor.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # NOTE: Utilizamos para los cálculos bfloat16; cambiar a float16 en arquitecturas no Ampere.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=qlora_config, token=True)\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoraLayer):\n",
    "        module = module.to(torch.bfloat16)\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "    if \"lm_head\" in name or \"embed_tokens\" in name:\n",
    "        if hasattr(module, \"weight\"):\n",
    "            module = module.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del Trainer y Entrenamiento\n",
    "\n",
    "Se inicializa el `Trainer` (en este caso un `SFTTrainer` específico para entrenamiento de modelos de lenguaje) con el modelo, los argumentos de entrenamiento, y el dataset formateado. Finalmente, se ejecuta el entrenamiento del modelo utilizando el método `.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=alpaca[\"train\"],\n",
    "    eval_dataset=alpaca[\"test\"],\n",
    "    formatting_func=format_chat,\n",
    "    max_seq_length=max_seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de LLMs con AutoTransformers.\n",
    "\n",
    "Ahora veremos cómo llevar a cabo esto mismo con `autotransformers`, que simplifica el proceso a la vez que ofrece mayor flexibilidad en cómo se procesan los datos y se lleva a cabo el entrenamiento. Esta parte es una adaptación a español de [este notebook](https://github.com/lenguajenatural-ai/autotransformers/blob/master/notebooks/chatbot_instructions/train_instructional_chatbot.ipynb), que tiene las explicaciones más completas desarrolladas originalmente en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alejandro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/alejandro/miniconda3/envs/test_autotransformers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/alejandro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from autotransformers import AutoTrainer, DatasetConfig, ModelConfig\n",
    "from autotransformers.llm_templates import instructions_to_chat, NEFTuneTrainer, QLoraWrapperModelInit, modify_tokenizer, qlora_config, SavePeftModelCallback\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando la Plantilla de Chat\n",
    "\n",
    "Para formatear correctamente las conversaciones para el entrenamiento, definimos una plantilla de chat usando la sintaxis de plantillas Jinja2. Esta plantilla itera a través de cada mensaje en una conversación, categorizándolos y formateándolos basados en su rol:\n",
    "\n",
    "- **Mensajes de Usuario**: Envueltos con etiquetas `<user>` para indicar claramente mensajes del usuario. Estos son las instrucciones o consultas dirigidas al chatbot.\n",
    "\n",
    "- **Mensajes del Sistema**: Encerrados dentro de etiquetas `<system>`, seguidos por saltos de línea para la legibilidad. Estos mensajes podrían incluir instrucciones generadas por el sistema o contexto que guía las respuestas del chatbot.\n",
    "\n",
    "- **Respuestas del Asistente**: Colocadas entre la conversación, después de las etiquetas `</user>` y marcadas con etiquetas `</assistant>` al final, junto con el token de fin de oración (EOS). Estas son las respuestas del chatbot o acciones tomadas en respuesta al mensaje del usuario, en cada intervención o turno en la conversación.\n",
    "\n",
    "- **Datos de Entrada**: Marcados con etiquetas `<input>` para distinguir cualquier entrada adicional o información contextual proporcionada al chatbot.\n",
    "\n",
    "Este formato estructurado es crucial para que el modelo entienda los diferentes componentes de una conversación, permitiéndole generar respuestas apropiadas basadas en el rol de cada mensaje.\n",
    "\n",
    "Típicamente, una conversación empezará con el mensaje del sistema, luego tendrá una entrada conteniendo contexto adicional para el asistente, y luego turnos de usuario-asistente, que pueden ser uno o más.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_TEMPLATE = \"\"\"{% for message in messages %}\n",
    "    {% if message['role'] == 'user' %}\n",
    "        {{'<user> ' + message['content'].strip() + ' </user>' }}\n",
    "    {% elif message['role'] == 'system' %}\n",
    "        {{'<system>\\\\n' + message['content'].strip() + '\\\\n</system>\\\\n\\\\n' }}\n",
    "    {% elif message['role'] == 'assistant' %}\n",
    "        {{ message['content'].strip() + ' </assistant>' + eos_token }}\n",
    "    {% elif message['role'] == 'input' %}\n",
    "        {{'<input> ' + message['content'] + ' </input>' }}\n",
    "    {% endif %}\n",
    "{% endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del Dataset\n",
    "\n",
    "La fase de preparación del dataset es crucial para estructurar los datos de manera que sea propicia para el entrenamiento de un chatbot. Primero cargamos el dataset desde el hub y luego utilizamos `instructions_to_chat`, para transformar cada muestra del dataset `somos-clean-alpaca` en un formato que refleje un flujo de conversación real involucrando un mensaje del sistema, la entrada del usuario y la respuesta del asistente.\n",
    "\n",
    "### La Función `instructions_to_chat`\n",
    "\n",
    "`instructions_to_chat` toma un diccionario que representa una sola muestra del dataset y lo reestructura categorizando y ordenando mensajes basados en su rol en una conversación:\n",
    "\n",
    "- Comienza agregando un **mensaje del sistema** que establece el contexto para el chatbot como un asistente diseñado para seguir las instrucciones del usuario.\n",
    "- Si está presente, los **datos de entrada** se agregan a continuación para proporcionar contexto o información adicional necesaria para cumplir con la solicitud del usuario.\n",
    "- La **instrucción del usuario** se añade luego, seguida de la **respuesta del asistente**, que es la respuesta a la solicitud del usuario.\n",
    "\n",
    "Esta reestructuración resulta en una lista `messages` dentro del diccionario de muestra, conteniendo todos los elementos de la conversación en su orden lógico.\n",
    "\n",
    "### Aplicando la Transformación\n",
    "\n",
    "Para aplicar esta transformación a través de todo el dataset:\n",
    "\n",
    "- Utilizamos el método `.map` con `instructions_to_chat` como la función de mapeo, estableciendo `batched=False` para procesar las muestras individualmente y `num_proc=4` para paralelizar la operación, mejorando la eficiencia.\n",
    "- Se eliminan las columnas que no forman parte de la estructura de `messages` para simplificar el dataset.\n",
    "\n",
    "Finalmente, el dataset se divide en conjuntos de entrenamiento y prueba con un 20% para el tamaño de prueba, asegurando que podamos evaluar el rendimiento de nuestro chatbot en datos no vistos. Esta división se logra usando el método `train_test_split`, proporcionando una base sólida para entrenar y validar el modelo del chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = load_dataset(\"somosnlp/somos-clean-alpaca-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = alpaca.map(\n",
    "    partial(\n",
    "        instructions_to_chat,\n",
    "        input_field=\"1-instruction\",\n",
    "        context_field=\"2-input\",\n",
    "        output_field=\"3-output\",\n",
    "        nested_field=\"inputs\",\n",
    "        system_message=\"Eres un asistente que resuelve las instrucciones que le presenta el usuario. En caso de tener un contexto adicional, utilízalo para resolver la instrucción.\"\n",
    "    ),\n",
    "    batched=False,\n",
    "    num_proc=4,\n",
    "    remove_columns=[col for col in alpaca[\"train\"].column_names if col != \"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = alpaca[\"train\"].train_test_split(0.2, seed=203984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando el Dataset para AutoTransformers\n",
    "\n",
    "Para asegurar que nuestro modelo de chatbot instructivo se entrene de manera eficiente y efectiva, configuramos meticulosamente nuestro dataset usando la configuración de dataset (`DatasetConfig`) de la biblioteca `autotransformers`. Este paso es esencial para adaptar el proceso de entrenamiento a nuestras necesidades específicas, incluyendo la configuración de hiperparámetros, detalles del dataset y estrategias de entrenamiento.\n",
    "\n",
    "### Configuración de los Argumentos de Entrenamiento\n",
    "\n",
    "Se define un conjunto de argumentos de entrenamiento fijos (`fixed_train_args`) para controlar varios aspectos del proceso de entrenamiento:\n",
    "\n",
    "- **Tamaños de lote** tanto para el entrenamiento como para la evaluación se establecen en 1, indicando que las muestras se procesan individualmente. Esto puede ser particularmente útil para modelos grandes o cuando la memoria GPU es limitada.\n",
    "- **Acumulación de gradientes** se utiliza con 16 pasos, permitiéndonos simular efectivamente un tamaño de lote más grande y estabilizar el entrenamiento sin exceder los límites de memoria.\n",
    "- Un **ratio de calentamiento** de 0.03 aumenta gradualmente la tasa de aprendizaje al comienzo del entrenamiento para prevenir que el modelo converja demasiado rápido a una solución subóptima.\n",
    "- **Tasa de aprendizaje**, **decaimiento de peso**, y otros ajustes de optimización son cuidadosamente elegidos para equilibrar la velocidad de aprendizaje del modelo y la calidad.\n",
    "- **Estrategias de evaluación y guardado** se configuran para verificar periódicamente el rendimiento del modelo y guardar puntos de control, permitiendo el monitoreo y la continuación del entrenamiento desde el último estado guardado.\n",
    "\n",
    "### Creando la Configuración del Dataset\n",
    "\n",
    "El diccionario `alpaca_config` abarca toda la información necesaria para la preparación e integración del dataset:\n",
    "\n",
    "- **Detalles del dataset** como el nombre, tipo de tarea y columnas específicas a usar para texto y etiquetas aseguran que el modelo se entrene en el formato correcto de datos.\n",
    "- **Parámetros de entrenamiento** se incluyen a través del diccionario `fixed_training_args`.\n",
    "- **Clases de callback**, como `SavePeftModelCallback`, automatizan pasos importantes como el guardado del modelo durante el entrenamiento.\n",
    "- **Optimizaciones de proceso** como establecer una semilla para reproducibilidad, especificar la dirección de optimización y la métrica, y habilitar divisiones parciales para la creación del conjunto de validación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_train_args = {\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"bf16\": True,\n",
    "    \"logging_steps\": 50,\n",
    "    \"lr_scheduler_type\": \"constant\",\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"eval_steps\": 200,\n",
    "    \"save_steps\": 50,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"logging_first_step\": True,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \"optim\": \"paged_adamw_32bit\",\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"group_by_length\": False,\n",
    "    \"save_total_limit\": 50,\n",
    "    \"adam_beta2\": 0.999\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_config = {\n",
    "    \"seed\": 9834,\n",
    "    \"callbacks\": [SavePeftModelCallback],\n",
    "    \"fixed_training_args\": fixed_train_args,\n",
    "    \"dataset_name\": \"alpaca\",\n",
    "    \"alias\": \"alpaca\",\n",
    "    \"retrain_at_end\": False,\n",
    "    \"task\": \"chatbot\",\n",
    "    \"text_field\": \"messages\",\n",
    "    \"label_col\": \"messages\",\n",
    "    \"num_proc\": 4, # \n",
    "    \"loaded_dataset\": alpaca, # Aquí metemos el dataset pre-cargado.\n",
    "    \"partial_split\": True, # NOTE: Para crear una partición de validación.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_config = DatasetConfig(**alpaca_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración del Modelo\n",
    "\n",
    "En la sección \"Configuración del Modelo\", delineamos cómo configurar las configuraciones del modelo usando `autotransformers`, enfocándonos en integrar LoRA (Adaptación de Bajo Rango) para la adaptación del modelo y aplicar la cuantización para la eficiencia. Estos pasos son cruciales para personalizar el modelo para nuestra tarea y entorno específicos, asegurando un rendimiento óptimo y la utilización de recursos.\n",
    "\n",
    "### Configuración de LoRA\n",
    "\n",
    "El objeto `LoraConfig` se instancia con parámetros diseñados para mejorar la adaptabilidad del modelo mientras se mantiene la eficiencia:\n",
    "\n",
    "- **r (rango)** y **lora_alpha** se establecen para ajustar la capacidad y el multiplicador de la tasa de aprendizaje para las capas LoRA, equilibrando entre la flexibilidad del modelo y el riesgo de sobreajuste.\n",
    "- **target_modules** especifica qué partes del modelo aplicar LoRA. En este caso, se apuntan los módulos \"all-linear\" para la adaptación, ofreciendo una mejora amplia sobre las capacidades del modelo.\n",
    "- **lora_dropout** se ajusta según el tamaño del modelo, asegurando que la regularización esté escalada apropiadamente.\n",
    "- La configuración de **bias** se establece en \"none\", indicando que no se usan términos de bias adicionales en las capas de adaptación LoRA.\n",
    "- El **task_type** se especifica como \"CAUSAL_LM\" para indicar la tarea de modelado del lenguaje causal, alineándose con la naturaleza del chatbot instructivo.\n",
    "- El parámetro **use_rslora** se utiliza para activar rank-stabilized lora que nos permite entrenar con rangos más altos.\n",
    "\n",
    "### Configuración del Modelo GEMMA\n",
    "\n",
    "La `ModelConfig` para el modelo GEMMA incluye varios parámetros clave y personalizaciones:\n",
    "\n",
    "- **Nombre del Modelo**: Especifica el modelo preentrenado a ser adaptado, \"google/gemma-2b-it\" en este caso.\n",
    "- **Nombre de Guardado y Directorio**: Define la convención de nomenclatura y ubicación para guardar el modelo afinado.\n",
    "- **Parámetros Personalizados**: Incluye configuraciones específicas del modelo, como habilitar la confianza en código remoto y configurar el mapeo de dispositivos para el entrenamiento.\n",
    "- **Envoltorio de Inicialización del Modelo**: `QLoraWrapperModelInit` se usa para integrar el marco de cuantización QLoRA con el modelo configurado LoRA, optimizando tanto la adaptabilidad como la eficiencia.\n",
    "- **Configuraciones de Cantidadización y PEFT**: Se aplican a través de los parámetros `quantization_config` y `peft_config`, asegurando que el modelo se beneficie tanto de las adaptaciones LoRA como de la cuantización eficiente después del entrenamiento.\n",
    "- **Modificación del Tokenizador**: Se usa una función parcial para personalizar el tokenizador, ajustando la longitud de secuencia, añadiendo tokens especiales e incorporando la plantilla de chat diseñada para nuestro contexto conversacional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64, # NOTE: Al usar rslora podemos subir el rango con mejoras en el rendimiento.\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\", # NOTE: En QLoRA entrenamos todas las capas lineales del modelo.\n",
    "    lora_dropout=0.10,  # NOTE: 0.1 for <13B models, 0.05 otherwise.\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_rslora=True # NOTE: flag para usar QLoRA.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_config = ModelConfig(\n",
    "    name=\"google/gemma-2b-it\",\n",
    "    save_name=\"gemma_2b\",\n",
    "    save_dir=\"./gemma_2b_alpaca\",\n",
    "    model_init_wrap_cls=QLoraWrapperModelInit,\n",
    "    quantization_config=qlora_config,\n",
    "    peft_config=lora_config,\n",
    "    neftune_noise_alpha=10, # NOTE: Este es el parámetro que podemos tocar de NEFTune.\n",
    "    custom_trainer_cls=NEFTuneTrainer, # NOTE: Un Trainer ajustado para usar NEFTune.\n",
    "    func_modify_tokenizer=partial(\n",
    "        modify_tokenizer,\n",
    "        new_model_seq_length=4096, # lower the maximum seq length to 4096 instead of 8192 to fit in google colab GPUs.\n",
    "        add_special_tokens={\"pad_token\": \"[PAD]\"}, # add pad token.\n",
    "        chat_template=CHAT_TEMPLATE # add the new chat template including the system and input roles.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a Entrenar\n",
    "\n",
    "Con nuestras configuraciones de dataset y modelo en su lugar, ahora estamos listos para iniciar el proceso de entrenamiento. Aquí es donde entra en juego la clase `AutoTrainer` de la biblioteca `autotransformers`, orquestando toda la operación de entrenamiento basada en las especificaciones que hemos proporcionado.\n",
    "\n",
    "### Configurando el AutoTrainer\n",
    "\n",
    "El `AutoTrainer` es una clase integral diseñada para agilizar el entrenamiento de modelos de aprendizaje automático, especialmente adaptada para modelos de lenguaje grandes. Acepta varios parámetros para controlar el proceso de entrenamiento:\n",
    "\n",
    "- **Configuraciones del Modelo**: Una lista de objetos `ModelConfig`, cada uno definiendo las configuraciones y personalizaciones para un modelo. Para nuestro chatbot instructivo, incluimos la configuración para el modelo GEMMA adaptado con LoRA y cuantización.\n",
    "- **Configuraciones del Dataset**: Similar a las configuraciones del modelo, estas se especifican usando objetos `DatasetConfig`. Pasamos la configuración para nuestro dataset `alpaca` preprocesado y estructurado, asegurando que se utilice efectivamente durante el entrenamiento.\n",
    "- **Directorio de Métricas**: Especifica el directorio donde se almacenarán las métricas de entrenamiento, permitiendo el monitoreo y evaluación del rendimiento.\n",
    "- **Modo de Búsqueda de Hiperparámetros**: Establecido en \"fijo\" en nuestro caso, indicando que no estamos explorando diferentes hiperparámetros sino entrenando con un conjunto predeterminado.\n",
    "- **Limpieza**: Una bandera booleana para limpiar los datos de ejecuciones anteriores, asegurando un nuevo inicio para cada sesión de entrenamiento.\n",
    "- **Limpiador de Métricas**: Especifica la utilidad para manejar datos temporales de métricas, manteniendo nuestro directorio de métricas ordenado y centrado en resultados significativos.\n",
    "- **Usar Token de Autenticación**: Habilita el uso de un token de autenticación, necesario para acceder a ciertos modelos o datasets que pueden tener restricciones de acceso.\n",
    "\n",
    "### Iniciando el Entrenamiento\n",
    "\n",
    "Con el `AutoTrainer` configurado, procedemos a llamar a su método de ejecución. Este paso inicia el proceso de entrenamiento, aprovechando las configuraciones que hemos configurado meticulosamente. El proceso implica:\n",
    "\n",
    "- Cargar y preparar automáticamente el dataset según nuestro `DatasetConfig`.\n",
    "- Adaptar y afinar el modelo basado en el `ModelConfig`, incluyendo cualquier mejora de LoRA o cuantización especificada.\n",
    "- Evaluar regularmente el rendimiento del modelo usando el conjunto de validación proporcionado, permitiéndonos monitorear su efectividad en tiempo real.\n",
    "- Guardar puntos de control del modelo y métricas de entrenamiento, habilitando tanto la introspección del proceso de entrenamiento como la reanudación del entrenamiento desde el último estado guardado.\n",
    "\n",
    "Al completarse, los resultados del entrenamiento, incluyendo métricas de rendimiento y puntos de control del modelo, están disponibles para análisis y despliegue. Este paso marca la culminación de la preparación de nuestro chatbot instructivo, dejándolo listo para pruebas y eventualmente, despliegue en escenarios del mundo real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotrainer = AutoTrainer(\n",
    "    model_configs=[gemma_config], # NOTE: Aquí podríamos poner tantos modelos como quisiéramos, y se entrenarían en bucle.\n",
    "    dataset_configs=[alpaca_config], # NOTE: Aquí también podríamos utilizar tantos datasets como quisiéramos.\n",
    "    metrics_dir=\"./metrics_alpaca\",\n",
    "    hp_search_mode=\"fixed\",\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating over datasets...:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map (num_proc=4): 100%|██████████| 35320/35320 [00:05<00:00, 5994.24 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map (num_proc=4): 100%|██████████| 6233/6233 [00:02<00:00, 2563.99 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map (num_proc=4): 100%|██████████| 10389/10389 [00:03<00:00, 3419.05 examples/s]\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]\n",
      "/home/alejandro/miniconda3/envs/test_autotransformers/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='2207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  18/2207 01:02 < 2:22:47, 0.26 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying models on dataset alpaca:   0%|          | 0/1 [01:31<?, ?it/s]\n",
      "Iterating over datasets...:   0%|          | 0/1 [01:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mautotrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/Usuario/Documents/autotransformers/src/autotransformers/autotrainer.py:102\u001b[0m, in \u001b[0;36mAutoTrainer.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptuna_hp_search()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_search_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_with_fixed_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/Usuario/Documents/autotransformers/src/autotransformers/autotrainer.py:171\u001b[0m, in \u001b[0;36mAutoTrainer.train_with_fixed_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compute_metrics_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_compute_metrics(dataset_config)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m transformers_manager\u001b[38;5;241m.\u001b[39mload_trainer(\n\u001b[1;32m    163\u001b[0m         dataset,\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         config,\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_model_fixed_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_metrics_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m fix_eval_results_dict(test_results)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/c/Users/Usuario/Documents/autotransformers/src/autotransformers/autotrainer.py:302\u001b[0m, in \u001b[0;36mAutoTrainer.train_one_model_fixed_params\u001b[0;34m(self, model_config, dataset_config, compute_metrics_func, test_dataset)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03mTrain one model with fixed params in one dataset, without tuning parameters.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m    Dictionary with results over the test set after training with fixed params.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39monly_test:\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_test_results(\n\u001b[1;32m    305\u001b[0m     dataset_config, compute_metrics_func, model_config, test_dataset\n\u001b[1;32m    306\u001b[0m )\n\u001b[1;32m    307\u001b[0m _save_metrics(\n\u001b[1;32m    308\u001b[0m     test_results,\n\u001b[1;32m    309\u001b[0m     model_config\u001b[38;5;241m.\u001b[39msave_name,\n\u001b[1;32m    310\u001b[0m     dataset_config\u001b[38;5;241m.\u001b[39malias,\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_dir,\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Users/Usuario/Documents/autotransformers/src/autotransformers/llm_templates.py:227\u001b[0m, in \u001b[0;36mNEFTuneTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(Trainer\u001b[38;5;241m.\u001b[39mtrain)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    Start the training loop, with an optional integration of Neftune noise into the model's input embeddings. After training, it ensures that the model's input embeddings are restored to their original forward pass method, effectively removing the Neftune noise injection.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    - This design allows the temporary integration of noise for experimental or augmentation purposes without permanently altering the model's behavior.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# for the embedding layer\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/test_autotransformers/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_autotransformers/lib/python3.10/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/test_autotransformers/lib/python3.10/site-packages/transformers/trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2909\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/test_autotransformers/lib/python3.10/site-packages/accelerate/accelerator.py:1966\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1966\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_autotransformers/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_autotransformers/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = autotrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotransformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
