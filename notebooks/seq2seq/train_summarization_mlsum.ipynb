{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lenguajenatural-ai/autotransformers/blob/main/notebooks/NER/train_spanish_ner.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Seq2Seq Learning with autotransformers: Mastering Text Summarization\n",
    "\n",
    "Welcome to this comprehensive tutorial on sequence-to-sequence (seq2seq) learning using autotransformers, with a special focus on text summarization. Seq2seq models have revolutionized the way we approach various natural language processing (NLP) tasks, offering powerful tools to handle problems that involve converting one sequence of data into another. These models are at the heart of numerous applications, from machine translation and text summarization to question-answering and chatbot development.\n",
    "\n",
    "Text summarization, the process of distilling the most important information from a source text to produce a shorter, concise version, serves as an exemplary case study to understand and harness the power of seq2seq models. This task not only demonstrates the model's ability to comprehend and generate text but also showcases its potential in extracting and condensing information, which is crucial for both academic research and real-world applications.\n",
    "\n",
    "In this tutorial, we'll guide you through the steps of training a seq2seq model for text summarization using autotransformers, autotransformers's comprehensive library designed to streamline the development and training of language models. We'll cover everything from data preparation and model selection to training strategies and evaluation metrics. By the end of this tutorial, you'll have a solid foundation in seq2seq learning, equipped with the knowledge and skills to adapt the techniques learned here to a wide range of seq2seq tasks beyond summarization, such as machine translation, text generation, and more.\n",
    "\n",
    "Our goal is to not only provide you with theoretical knowledge but also hands-on experience, ensuring that you're well-prepared to tackle seq2seq challenges with confidence. Let's embark on this journey together, unlocking the full potential of seq2seq models with autotransformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/miniconda3/envs/chatnatural/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/alejandro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alejandro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from autotransformers import AutoTrainer, DatasetConfig, ModelConfig, ResultsPlotter\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import Seq2SeqTrainer, MT5ForConditionalGeneration, XLMProphetNetForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the dataset\n",
    "\n",
    "For training sequence-to-sequence (seq2seq) tasks such as text summarization with autotransformers, the initial step involves defining the dataset configuration along with the training arguments. These configurations play a critical role in customizing the training process, ensuring that it is optimized for the specific requirements of the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Training Arguments\n",
    "\n",
    "The first component of the dataset configuration is the `fixed_train_args`. This dictionary encapsulates a set of `transformers.TrainingArguments` that are later passed to the `transformers.Trainer` within `autotransformers.AutoTrainer`. The `TrainingArguments` class provides a comprehensive range of options to fine-tune the training behavior, from the evaluation strategy to the model's saving behavior.\n",
    "\n",
    "Here is an overview of the key training arguments we're setting:\n",
    "\n",
    "- `evaluation_strategy`: \"epoch\" - Evaluates the model performance at the end of each epoch.\n",
    "- `num_train_epochs`: 10 - Specifies the total number of training epochs.\n",
    "- `do_train`: True - Enables the training process.\n",
    "- `do_eval`: True - Enables the evaluation process.\n",
    "- `logging_strategy`: \"epoch\" - Logs metrics at the end of each epoch.\n",
    "- `save_strategy`: \"epoch\" - Saves the model at the end of each epoch.\n",
    "- `save_total_limit`: 2 - Limits the total number of model checkpoints to save.\n",
    "- `seed`: 69 - Sets the seed for generating random numbers.\n",
    "- `bf16`: True - Enables bfloat16 mixed precision training for faster computation.\n",
    "- `dataloader_num_workers`: 16 - Sets the number of subprocesses to use for data loading.\n",
    "- `load_best_model_at_end`: True - Loads the best model found during training when training is finished.\n",
    "- `optim`: adafactor - Uses the Adafactor optimizer instead of AdamW, which is more memory efficient.\n",
    "\n",
    "For a comprehensive list of available training arguments, refer to the [TrainingArguments documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_train_args = {\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 2,\n",
    "    \"seed\": 69,\n",
    "    \"bf16\": True,\n",
    "    \"dataloader_num_workers\": 16,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"optim\": \"adafactor\",\n",
    "    \"max_steps\": 1 # NOTE: This is added for the purpose of the tutorial.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Configuration for Text Summarization\n",
    "\n",
    "After establishing the training arguments, we define the `mlsum_config` dictionary, which outlines the specific settings for the text summarization task using the MLSum dataset. This configuration includes both the `fixed_train_args` and additional parameters tailored to the dataset and task:\n",
    "\n",
    "- `seed`: 44 - Seed for random number generation, ensuring reproducibility.\n",
    "- `direction_optimize`: \"maximize\" - The direction of optimization for the metric of interest.\n",
    "- `metric_optimize`: \"eval_rouge2\" - The metric to optimize during training, in this case, ROUGE-2 for summarization quality.\n",
    "- `callbacks`: A list of callback functions for training, such as `EarlyStoppingCallback` to prevent overfitting.\n",
    "- `fixed_training_args`: The dictionary of training arguments defined previously.\n",
    "\n",
    "Additional parameters specific to the MLSum dataset and the summarization task are also specified:\n",
    "\n",
    "- `dataset_name` and `alias`: Both set to \"mlsum\" for identification.\n",
    "- `retrain_at_end`: False - Indicates whether to retrain the model on the entire dataset after validation.\n",
    "- `task`: \"summarization\" - Specifies the NLP task.\n",
    "- `hf_load_kwargs`: Arguments for loading the dataset, including the path and name.\n",
    "- `label_col`: \"summary\" - Defines the column to use as the label for summarization.\n",
    "- `num_proc`: 16 - The number of processing threads for data preprocessing.\n",
    "\n",
    "Lastly, the `mlsum_config` dictionary is transformed into a `DatasetConfig` object, encapsulating all the necessary configurations for the dataset and training setup.\n",
    "\n",
    "This structured approach to configuring the dataset and training parameters ensures that you can adapt and optimize the seq2seq model training for text summarization, with the flexibility to adjust settings for other seq2seq tasks as well.\n",
    "\n",
    "To delve deeper into configuring training arguments and understanding their impact on model performance, consider exploring the Hugging Face Course on NLP, which offers extensive guidance on working with the Transformers library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlsum_config = {\n",
    "    \"seed\": 44,\n",
    "    \"direction_optimize\": \"maximize\",\n",
    "    \"metric_optimize\": \"eval_rouge2\",\n",
    "    \"callbacks\": [EarlyStoppingCallback(1, 0.00001)],\n",
    "    \"fixed_training_args\": fixed_train_args\n",
    "}\n",
    "\n",
    "mlsum_config.update(\n",
    "    {\n",
    "        \"dataset_name\": \"mlsum\",\n",
    "        \"alias\": \"mlsum\",\n",
    "        \"retrain_at_end\": False,\n",
    "        \"task\": \"summarization\",\n",
    "        \"hf_load_kwargs\": {\"path\": \"mlsum\", \"name\": \"es\"},\n",
    "        \"label_col\": \"summary\",\n",
    "        \"num_proc\": 16}\n",
    ")\n",
    "\n",
    "mlsum_config = DatasetConfig(**mlsum_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Configuration\n",
    "\n",
    "In this section, we define the hyperparameter search space and preprocessing functions, followed by the configuration for each model we plan to evaluate. The goal is to find the optimal set of parameters that yields the best performance on our seq2seq task of text summarization. We will explore different models to demonstrate the versatility of autotransformers in handling various architectures efficiently.\n",
    "\n",
    "### Hyperparameter Search Space\n",
    "\n",
    "The `hp_space` function is designed to define the hyperparameter search space for the optimization process. This function takes a `trial` object as input and returns a dictionary mapping hyperparameter names to their suggested values. We use `suggest_categorical` for simplicity, specifying discrete choices for each hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\n",
    "            \"learning_rate\", [3e-5, 5e-5, 7e-5, 2e-4]\n",
    "        ),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\n",
    "            \"num_train_epochs\", [10]\n",
    "        ),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\n",
    "            \"per_device_train_batch_size\", [8]),\n",
    "        \"per_device_eval_batch_size\": trial.suggest_categorical(\n",
    "            \"per_device_eval_batch_size\", [8]),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\n",
    "            \"gradient_accumulation_steps\", [8]),\n",
    "        \"warmup_ratio\": trial.suggest_categorical(\n",
    "            \"warmup_ratio\", [0.08]\n",
    "        ),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function\n",
    "Before feeding the data to our models, we need to preprocess it. The preprocess_function tokenizes the input text and labels (summaries), truncating them to fit the model's maximum input length. It also converts the labels into model input IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_function(examples, tokenizer, dataset_config):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[dataset_config.text_field],\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[dataset_config.summary_field],\n",
    "        max_length=dataset_config.max_length_summary,\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations\n",
    "We then define configurations for each model we intend to train. Here, we showcase configurations for `mt5-large` and `xprophetnet-large-wiki100-cased`, specifying the model's name, the hyperparameter search space, and the preprocessing function, among other settings. Each configuration is encapsulated in a ModelConfig object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mt5_config = ModelConfig(\n",
    "    name=\"google/mt5-large\",\n",
    "    save_name=\"mt5-large\",\n",
    "    hp_space=hp_space,\n",
    "    num_beams=4,\n",
    "    trainer_cls_summarization=Seq2SeqTrainer,\n",
    "    model_cls_summarization=MT5ForConditionalGeneration,\n",
    "    custom_tokenization_func=preprocess_function,\n",
    "    n_trials=1,\n",
    "    random_init_trials=1\n",
    ")\n",
    "xprophetnet_config = ModelConfig(\n",
    "    name=\"microsoft/xprophetnet-large-wiki100-cased\",\n",
    "    save_name=\"xprophetnet\",\n",
    "    hp_space=hp_space,\n",
    "    num_beams=4,\n",
    "    trainer_cls_summarization=Seq2SeqTrainer,\n",
    "    model_cls_summarization=XLMProphetNetForConditionalGeneration,\n",
    "    custom_tokenization_func=preprocess_function,\n",
    "    n_trials=1,\n",
    "    random_init_trials=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These configurations are crucial for setting up our experiments, allowing us to systematically evaluate and compare the performance of different models on the summarization task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "\n",
    "After configuring our models and the dataset, we proceed to instantiate the `AutoTrainer` class. This powerful class from autotransformers orchestrates the training and evaluation process for the specified models and datasets. It's designed to streamline the experimentation process, making it easier to compare the performance of different model configurations across various tasks.\n",
    "\n",
    "### Setting Up AutoTrainer\n",
    "\n",
    "The `AutoTrainer` is initialized with the following key components:\n",
    "\n",
    "- `model_configs`: A list of model configurations to be trained and evaluated. In our case, we include the configurations for `mt5-large` and `xprophetnet-large-wiki100-cased`.\n",
    "- `dataset_configs`: A list containing the dataset configurations. Here, it includes our earlier defined `mlsum_config` for the text summarization task.\n",
    "- `metrics_dir`: The directory path where the evaluation metrics for each model will be saved. We specify `\"mlsum_multilingual_models\"` to organize our results.\n",
    "- `metrics_cleaner`: The function or script used to process and clean the metrics data. We use `\"metrics_mlsum\"` to ensure our results are formatted correctly and easily interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotrainer = AutoTrainer(\n",
    "    model_configs=[mt5_config, xprophetnet_config],\n",
    "    dataset_configs=[mlsum_config],\n",
    "    metrics_dir=\"mlsum_multilingual_models\",\n",
    "    metrics_cleaner=\"metrics_mlsum\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Training and Evaluation Process\n",
    "With the `AutoTrainer` configured, we simply call it to start the training and evaluation process across our specified models and dataset. The results, including performance metrics for each model, are captured and printed out, providing a comprehensive overview of how each model performed on the summarization task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = autotrainer()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process not only facilitates an efficient way to train and evaluate multiple models but also organizes and presents the results in a manner that aids in decision-making for selecting the best-performing model for your specific NLP task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Model Performance\n",
    "\n",
    "After training and evaluating our models, it's important to visualize their performance to make informed decisions. autotransformers provides a convenient way to do this through the `ResultsPlotter` class, which generates comparative plots of model metrics across different configurations. This visualization helps in understanding the strengths and weaknesses of each model in a more intuitive manner.\n",
    "\n",
    "### Setting Up ResultsPlotter\n",
    "\n",
    "The `ResultsPlotter` is initialized with several parameters to specify the source of the metrics data and how it should be visualized:\n",
    "\n",
    "- `metrics_dir`: The directory where the metrics are stored. We use `autotrainer.metrics_dir` to automatically fetch the directory specified during the AutoTrainer setup.\n",
    "- `model_names`: A list of model names to include in the plot. This is dynamically generated from the model configurations used in the AutoTrainer, ensuring that all evaluated models are represented.\n",
    "- `dataset_to_task_map`: A mapping of dataset aliases to their respective tasks, helping in the categorization and labeling of plot data. This mapping is constructed from the dataset configurations used in the AutoTrainer.\n",
    "- `metric_field`: The specific metric to be plotted. We choose `\"rouge2\"` as it's a common metric for evaluating text summarization models, reflecting their ability to generate coherent and concise summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotter = ResultsPlotter(\n",
    "    metrics_dir=autotrainer.metrics_dir,\n",
    "    model_names=[model_config.save_name for model_config in autotrainer.model_configs],\n",
    "    dataset_to_task_map={dataset_config.alias: dataset_config.task for dataset_config in autotrainer.dataset_configs},\n",
    "    metric_field=\"rouge2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating and Saving the Plot\n",
    "With the `ResultsPlotter` configured, we generate the metrics plot by calling `plot_metrics()`. This method returns a matplotlib Axes object, which we can then use to further customize the plot or save it directly. Here, we save the plot as \"results.png\", providing a visual summary of our models' performance on the text summarization task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plotter.plot_metrics()\n",
    "ax.figure.savefig(\"results.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatnatural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
